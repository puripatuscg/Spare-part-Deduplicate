{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675ea1b8",
   "metadata": {},
   "source": [
    "# Spart-part Deduplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b2fc64",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebdc0e",
   "metadata": {},
   "source": [
    "Spare-Part Deduplication ด้วย Hybrid RAG และ Graph-based Grouping\n",
    "\n",
    "โปรเจค Spare-Part Deduplication มีวัตถุประสงค์เพื่อแก้ไขปัญหา material / spare-part ที่ซ้ำกันแต่มีหลายรหัส (PK) ซึ่งเกิดจากการบันทึกข้อมูลจากหลายแหล่ง หลายโรงงาน หรือหลายช่วงเวลา ทำให้ material ที่เป็นชิ้นส่วนเดียวกัน ถูกสร้างเป็นหลาย PK ส่งผลต่อความซับซ้อนของระบบจัดซื้อ คลังสินค้า และการวิเคราะห์ข้อมูลในภาพรวม\n",
    "\n",
    "แนวคิดและแนวทางแก้ไข (Solution Approach)\n",
    "\n",
    "โซลูชันของโปรเจคนี้ใช้แนวคิด Retrieval-Augmented Generation (RAG) ในรูปแบบ Hybrid Search เพื่อดึง candidate ที่มีความเป็นไปได้สูงว่าจะเป็น material เดียวกัน จากนั้นใช้ Large Language Model (LLM) เป็นตัวตัดสินเชิงความหมาย (semantic reasoning) ว่า candidate ใดควรถูกจัดเป็น duplicate\n",
    "\n",
    "ระบบแบ่งการทำงานออกเป็น 3 ขั้นตอนหลัก:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2edec",
   "metadata": {},
   "source": [
    "1. Hybrid Search (Candidate Retrieval)\n",
    "\n",
    "ในขั้นตอนแรก ระบบจะดึง candidate material จากฐานข้อมูล โดยใช้การค้นหาแบบผสม (Hybrid Search) ประกอบด้วย:\n",
    "\n",
    "Semantic Search (Vector Search)\n",
    "ใช้ embedding เพื่อจับความหมายของคำอธิบาย material แม้ข้อความจะไม่ตรงกันทุกคำ\n",
    "\n",
    "Keyword Search (BM25 / Full-text search)\n",
    "ใช้สำหรับจับ keyword สำคัญ เช่น ประเภทของชิ้นส่วน รหัสรุ่น หรือคำทางเทคนิค\n",
    "\n",
    "Trigram Fuzzy Matching\n",
    "ใช้ตรวจจับความคล้ายของข้อความในเชิงตัวอักษร เหมาะกับกรณีสะกดต่างกันเล็กน้อยหรือมีสัญลักษณ์พิเศษ\n",
    "\n",
    "ผลลัพธ์จากแต่ละวิธีจะถูก normalize และถ่วงน้ำหนัก (weighted scoring) เพื่อคัดเลือก candidate ที่มีแนวโน้มสูงสุดสำหรับขั้นตอนถัดไป"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59d258",
   "metadata": {},
   "source": [
    "2. LLM-based Grouping (Duplicate Decision)\n",
    "\n",
    "เมื่อได้ candidate จาก Hybrid Search แล้ว ระบบจะส่งข้อมูลเข้าสู่ LLM เพื่อทำการวิเคราะห์เชิงความหมายและบริบท โดยให้ LLM ตัดสินใจว่า:\n",
    "\n",
    "material ใดเป็น duplicate ของกันและกัน\n",
    "\n",
    "material ใดควรถูกแยกเป็นคนละกลุ่ม\n",
    "\n",
    "LLM จะช่วยจัดกลุ่ม material ที่มีความหมายเหมือนกัน แม้รูปแบบคำอธิบายหรือโครงสร้างข้อมูลจะแตกต่างกัน"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88180027",
   "metadata": {},
   "source": [
    "3. Graph-based Connectivity Grouping\n",
    "\n",
    "ผลลัพธ์จาก LLM จะถูกนำไปสร้างเป็น Graph โดย:\n",
    "\n",
    "material แต่ละ PK คือ node\n",
    "\n",
    "ความสัมพันธ์ว่า “เป็น duplicate กัน” คือ edge\n",
    "\n",
    "ระบบจะใช้แนวคิด connected components เพื่อรวมกลุ่ม duplicate ให้ครบถ้วน เช่น:\n",
    "\n",
    "ถ้า\n",
    "\n",
    "A = B\n",
    "\n",
    "B = C\n",
    "\n",
    "เมื่อนำเข้าสู่ graph จะได้กลุ่มเดียวกันคือ\n",
    "A = B = C\n",
    "\n",
    "วิธีนี้ช่วยให้ได้ duplicate group ที่สมบูรณ์ แม้ความสัมพันธ์จะไม่ได้ถูกค้นพบทั้งหมดในครั้งเดียว"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba31927",
   "metadata": {},
   "source": [
    "diagram flow: (RAG search → LLM → Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82157c44",
   "metadata": {},
   "source": [
    "ผลลัพธ์ที่คาดหวัง\n",
    "\n",
    "ลดจำนวน PK ที่ซ้ำซ้อนในระบบ\n",
    "\n",
    "เพิ่มความถูกต้องของ master data\n",
    "\n",
    "ลดต้นทุนจากการจัดซื้อซ้ำหรือการจัดเก็บชิ้นส่วนซ้ำ\n",
    "\n",
    "วางรากฐานสำหรับ data governance และ master data management ในระยะยาว"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5c6a9",
   "metadata": {},
   "source": [
    "## Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14095d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf3b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042ba17",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fe40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "po = pd.read_csv(\"Vw_POText.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bcee7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Material",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f77bcdbd-27eb-4f7b-b317-f9c6d2595402",
       "rows": [
        [
         "0",
         "210STKR0000018",
         "STK.PE ใสหลังกลาสซีน กาวธรรมดา 180mmx1000m #BW0065 ห้ามใช้"
        ],
        [
         "1",
         "225107A400004228",
         " CK SP  CK SP33131ORANGE nan ORANGE 4228 nan nan"
        ],
        [
         "2",
         "2503",
         "PLC, MFR PART NAME : PLC CPU, MFR PART NO. : 6ES7277-0AA22-0XA0, MFR NAME : SIMENS S7-200, REMARK : PLC CPU 6ES7277-0AA22-0XA0  SI"
        ],
        [
         "3",
         "25032A000045001",
         "FUSE, SHORT NAME  : HCR FUSE 24KV/100A, VOLTAGE RATING  : 24KV, CURRENT RATING  : 100A, MFR PART NO.  : 3GD1420-4D, MFR NAME  : SIEMENS, REMARK  : HCR FUSE 24KV/100A / DIMENSION E442 MM.   สถานที่ใช้งาน SUBSTRATION ,  ชื่อเครื่องจักร RMU (100) ข้อกำหนดการตรวจรับวัสดุ ต้องมีคู่มือการใช้งานอยุ่ในสภาพดี การบรรจุเรียบร้อย"
        ],
        [
         "4",
         "25032A000045002",
         "SENSOR S30B-2011DA#1026822 สถานที่ใช้งาน TRANSFER CAR WSA"
        ],
        [
         "5",
         "25032A110041001",
         "MOTOR, MFR PART NAME  : MOTOR, MFR PART NO.  : WFG0645-L, MFR NAME  : MITSUBISHI, REMARK  : MOTOR WFG0645-L ชื่อเครื่องจักร CORRUGATOR 2 (G/M)"
        ],
        [
         "6",
         "25032A110041002",
         "MOTOR WGE1550-R ชื่อเครื่องจักร CORRUGATOR 2 (G/M)"
        ],
        [
         "7",
         "25032A110041003",
         "AC SERVO MOTOR WHR3093-B ชื่อเครื่องจักร CORRUGATOR 2 (SINGLE FACER)"
        ],
        [
         "8",
         "25032A110041004",
         "SERVO MOTOR, TYPE  : HC-KFS23K-S49 REMARK  : SERVO MOTOR P/N WHS5924-B ชื่อเครื่องจักร CORRUGAYOR (SS)"
        ],
        [
         "9",
         "25032A110041005",
         "SERVO MOTOR, TYPE  : MCA 19517-RS0B0-B28R-ST5FION-ROSU, REMARK  : SERVO MOTOR P/N: 101782 ชื่อเครื่องจักร CORRUGATOR (EL)"
        ],
        [
         "10",
         "25032A110041006",
         "EL AG2591 DRIVE GEAR, MACHINE CODE  : CORRUGATOR 2, EQUIPMENT TYPE  : DRIVE GEAR, MFR NAME  : MITSUBISHI, MFR MODEL NO.  : 013439, MFR PART NO.  : 013439, REMARK  : ชื่อเครื่องจักร CORRUGATOR (EL)"
        ],
        [
         "11",
         "25032A110041007",
         "SERVO MOTOR  HC-SFS 5024G1 ชื่อเครื่องจักร CORRUGATOR (STACKER)"
        ],
        [
         "12",
         "25032A110041008",
         "SERVO MOTOR HC-SFS 3524BG1H (1/6) ชื่อเครื่องจักร CORRUGATOR (STACKER)"
        ],
        [
         "13",
         "25032A110041009",
         "SERVO MOTOR HC-SFS 3524BG1H (1/17) ชื่อเครื่องจักร CORRUGATOR (STACKER)"
        ],
        [
         "14",
         "25032A110041010",
         "nan SERVO MOTOR HC-SFS 7024BG1H ชื่อเครื่องจักร CORRUGATOR (STACKER)"
        ],
        [
         "15",
         "25032A110041011",
         "SERVO MOTOR HA-LFS22K24B ชื่อเครื่องจักร CORRUGATOR (STACKER)"
        ],
        [
         "16",
         "25032A110041012",
         "SERVO MOTOR, TYPE  : HA-LFS601K, REMARK  : SERVO MOTOR \"MIYSUBISHI\" HA-LFS601K ชื่อเครื่องจักร CORRUGATOR (EDGE CUT)"
        ],
        [
         "17",
         "25032A110041013",
         "SERVO MOTOR, TYPE  : HC-KFS23K-S49, REMARK  : SERVO MOTOR HC-KFS23K-S49 ชื่อเครื่องจักร CORRUGATOR (SS)"
        ],
        [
         "18",
         "25032A110042001",
         "PLC, MFR PART NAME  : CPU UNIT, MFR PART NO.  : Q2ASHCPU-S1, MFR NAME  : MITSUBISHI, REMARK  : CPU MODULE MODEL : Q2ASHCPU-S1 เครื่องจักร CORRUGATOR NO.2 MIN = 1 , MAX = 1"
        ],
        [
         "19",
         "25032A110042002",
         "PLC, MFR PART NAME  : COUNTER UNIT, MFR PART NO.  : A1SD62D, MFR NAME  : MITSUBISHI, REMARK  : HIGH SPEED COUNTER MODULE MODEL : A1SD62D เครื่องจักร CORRUGATOR NO.2 MIN = 1 , MAX = 1"
        ],
        [
         "20",
         "25032A110042003",
         "PLC, MFR PART NAME  : INPUT UNIT, MFR PART NO.  : A1SX42, MFR NAME  : MITSUBISHI, REMARK  : INPUT MODULE MODEL : A1SX42 เครื่องจักร CORRUGATOR NO.2 MIN = 1 , MAX = 1"
        ],
        [
         "21",
         "25032A110042004",
         "PLC, MFR PART NAME  : OUTPUT UNIT, MFR PART NO.  : A1SY42P, MFR NAME  :MITSUBISHI, REMARK  : OUTPUT MODULE  : A1SY42P เครื่องจักร CORRUGATOR NO.2 MIN = 1 , MAX = 1"
        ],
        [
         "22",
         "25032A110042005",
         "PLC, MFR PART NAME  : CC-LINK UNIT, MFR PART NO.  : A1SJ61QBT11, MFR NAME  : MITSUBISHI, REMARK  : CC-LINK MODULE MODEL : A1SJ61QBT11 เครื่องจักร CORRUGATOR NO.2 MIN = 1 , MAX = 1"
        ],
        [
         "23",
         "25032A110042006",
         "PLC, MFR PART NAME  : COMMUNICATION UNIT, MFR PART NO.  : A1SJ71QC24N1, MFR NAME  : MITSUBISHI, REMARK  : SERIAL COMMUNICATION MODULE MODEL : A1SJ71QC24N1 เครื่องจักร CORRUGATOR NO.2 MIN = 1 , MAX = 1"
        ],
        [
         "24",
         "25032A110042007",
         "PLC, MFR PART NAME  : CONTROLLERS, MFR PART NO.  : FX2NC-64MT, MFR NAME  : MITSUBISHI, REMARK  : ชื่ิอเครื่องจักร CORRUGATOR 2 (TRANSFERCAR GODO)"
        ],
        [
         "25",
         "25032A110042008",
         "PLC, MFR PART NAME  : CONTROLLERS, MFR PART NO.  : FX2NC-32EX, MFR NAME  : MITSUBISHI, REMARK  : ชื่ิอเครื่องจักร CORRUGATOR 2 (TRANSFERCAR GODO)"
        ],
        [
         "26",
         "25032A110042009",
         "PLC, MFR PART NAME  : CONTROLLERS, MFR PART NO.  : FX2NC-232ADP, MFR NAME  : MITSUBISHI, REMARK  : ชื่ิอเครื่องจักร CORRUGATOR 2 (TRANSFERCAR GODO)"
        ],
        [
         "27",
         "25032A110042010",
         "PLC, MFR PART NAME  : CONTROLLERS, MFR PART NO.  : FX2NC-CNV-IF, MFR NAME  : MITSUBISHI, REMARK  : ชื่ิอเครื่องจักร CORRUGATOR 2 (TRANSFERCAR GODO)"
        ],
        [
         "28",
         "25032A110042011",
         "PLC, MFR PART NAME  : CONTROLLERS, MFR PART NO.  : FX2N-16CCL-M, MFR NAME  : MITSUBISHI, REMARK  : ชื่ิอเครื่องจักร CORRUGATOR 2 (TRANSFERCAR GODO)"
        ],
        [
         "29",
         "25032A110042012",
         "PLC, MFR PART NAME  : CC-LINK GATEWAY, MFR PART NO.  : AG42-C1SL, MFR NAME  : GODO, REMARK  : CC-LINK GATEWAY AG42-C1SL ชื่ิอเครื่องจักร CORRUGATOR 2 (TRANSFERCAR GODO)"
        ],
        [
         "30",
         "25032A110042013",
         "PLC, MFR PART NAME  : T-CONVERTER, MFR PART NO.  : A115T-T1, MFR NAME  : GODO, REMARK  : T-CONVERTER A115T-T1 ชื่ิอเครื่องจักร CORRUGATOR 2 (TRANSFERCAR GODO)"
        ],
        [
         "31",
         "25032A110042014",
         "PLC, MFR PART NAME  : T-CONVERTER, MFR PART NO.  : A115T-R1, MFR NAME  : GODO, REMARK  : T-CONVERTER A115T-R1 ชื่ิอเครื่องจักร CORRUGATOR 2 (TRANSFERCAR GODO)"
        ],
        [
         "32",
         "25032A110042015",
         "FLUTE DETECTOR (LASER) WGE6181-L, MACHINE CODE  : CORRUGATOR 2, EQUIPMENT TYPE  : FLUTE DETECTOR, MFR NAME  : MITSUBISHI, MFR MODEL NO.  : WGE6181-L, MFR PART NO.  : WGE6181-L, REMARK  : FLUTE DETECTOR (LASER) WGE6181-L ชื่อเครื่องจักร CORRUGATOR (GLUE MACHINE)"
        ],
        [
         "33",
         "25032A110042016",
         "SHORT NAME  : PLC, MFR PART NO.  : PC BOARD OVRC1-0003, MFR NAME  : CORR. MITSUBISHI,  REMARK : PC BOARD OVRC1-0003 (Sigma2000 DRY-END) สถานที่ใช้งาน  Sigma 2000 Dry-end ชื่อเครื่องจักร   CORRUGATOR"
        ],
        [
         "34",
         "25032A110042017",
         "PC BOARD OVSC1-0004A ORAWING NUMBER สถานที่ใช้งาน  Sigma 2000 Dry-end ชื่อเครื่องจักร   CORRUGATOR MIN = 1 MAX = 1"
        ],
        [
         "35",
         "25032A110042018",
         "SHORT NAME  : PCB, TYPE  : DIGITAL I/O MODULE, MFR PART NO.  : 013438, MFR NAME  : ERHARDT & LEIMER, MFR MODEL NO.  : LK 4002,  REMARK : LK 4002 Digital I/O (P/N 013438)EL"
        ],
        [
         "36",
         "25032A110043001",
         "VALVE,SOLENOID, SUPPLY VOLTAGE  : 24, MFR MODEL NO.  : KSO-G02-2BP-30-E-2T, MFR NAME  : DAIKIN, REMARK  : SOLENOID CONTROL VALVE P/N  KSO-G02-2BP-30-E-2T สถานที่ใช้งาน CORRUGATOR (SINGLE FACER) MIN=1 , MAX=1"
        ],
        [
         "37",
         "25032A110044001",
         "SERVO AMP, MFR MODEL NO.  : MR-J2S-22KB4, MFR NAME  : MITSUBISHI, REMARK  : SERVO AMP \"MITSUBISHI\" MR-J2S-22KB4 สถานที่ใช้งาน CORRUGATOR (STACKER) ข้อกำหนดการตรวจรับวัสดุ ต้องมีคู่มือการใช้งานอยุ่ในสภาพดี การบรรจุเรียบร้อย"
        ],
        [
         "38",
         "25032A110044002",
         "SERVO AMP, MFR MODEL NO.  : MR-J2S-700B4, MFR NAME  : MITSUBISHI, REMARK  : SERVO AMP \"MITSUBISHI\" MR-J2S-700B4 สถานที่ใช้งาน CORRUGATOR(STACKER) ข้อกำหนดการตรวจรับวัสดุ ต้องมีคู่มือการใช้งานอยุ่ในสภาพดี การบรรจุเรียบร้อย"
        ],
        [
         "39",
         "25032A110044003",
         "SERVO AMP, MFR MODEL NO.  : FR-BU2-H30K, MFR NAME  : MITSUBISHI, REMARK  : SERVO AMP \"MITSUBISHI\" FR-BU2-H30K สถานที่ใช้งาน CORRUGATOR (STACKER) ข้อกำหนดการตรวจรับวัสดุ ต้องมีคู่มือการใช้งานอยุ่ในสภาพดี การบรรจุเรียบร้อย"
        ],
        [
         "40",
         "25032A110044004",
         "SERVO AMP, MFR MODEL NO.  : MR-J2S-700A4-S294, MFR NAME  : MITSUBISHI, REMARK  : SERVO AMP \"MITSUBISHI\" MR-J2S-700A4-S294 สถานที่ใช้งาน CORRUGATOR(SF) ข้อกำหนดการตรวจรับวัสดุ ต้องมีคู่มือการใช้งานอยุ่ในสภาพดี การบรรจุเรียบร้อย"
        ],
        [
         "41",
         "25032A110044005",
         "RISISTOR, MFR MODEL NO.  : FR-BR-H30K, MFR NAME  : MITSUBISHI, REMARK  : RISISTOR \"MITSUBISHI\" FR-BR-H30K สถานที่ใช้งาน CORRUGATOR (STACKER) ข้อกำหนดการตรวจรับวัสดุ ต้องมีคู่มือการใช้งานอยุ่ในสภาพดี การบรรจุเรียบร้อย"
        ],
        [
         "42",
         "25032A110044006",
         "PSU, MFR MODEL NO.  : PS5R-SF24, MFR NAME  : IDEC, REMARK  : ชื่อเครื่องจักร CORRUGATOR (GODO)"
        ],
        [
         "43",
         "25032A110044007",
         "PSU, MFR MODEL NO.  : PS5R-SG24, MFR NAME  : IDEC, REMARK  : ชื่อเครื่องจักร CORRUGATOR (GODO)"
        ],
        [
         "44",
         "25032A110044008",
         "SHORT NAME  : PSU, MFR MODEL NO.  : EWS100-5, MFR NAME  : OMRON,  REMARK : Power Supply (EWS100-5)"
        ],
        [
         "45",
         "25032A110044009",
         "SHORT NAME  : PSU, MFR MODEL NO.  : EWS15-12, MFR NAME  : OMRON,  REMARK : Power Supply (EWS15-12)"
        ],
        [
         "46",
         "25032A110044010",
         "SHORT NAME  : PSU, MFR MODEL NO.  : JWS150-24, MFR NAME  : MITSUBISHI,  REMARK : Power Supply (JWS150-24)"
        ],
        [
         "47",
         "25032A110044011",
         "SHORT NAME  : PSU, MFR MODEL NO.  : JWS100-24, MFR NAME  : MITSUBISHI, APPLICATION  : SPARE PART FOR SIGMA 2000,  REMARK : Power Supply (JWS100-24)Sigma 2000"
        ],
        [
         "48",
         "25032A110044012",
         "DC 6001 DIGITAL CONTROLLER, MFR MODEL NO.  : P/N 013675, MFR NAME  : ?, REMARK  : DC 6001 DIGITAL CONTROLLER (P/N 013675)EL"
        ],
        [
         "49",
         "25032A110044013",
         "SHORT NAME  : DC 0140 Digital Controller 50 W, MFR MODEL NO.  : P/N 013425 EL, MFR NAME  : EL, POWER RATING  : 50w, MFR PART NO.  : P/N 013425 EL,  REMARK : DC 0140 Digital Controller 50 WP/N 013425 EL"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 177929
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Material</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210STKR0000018</td>\n",
       "      <td>STK.PE ใสหลังกลาสซีน กาวธรรมดา 180mmx1000m #BW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>225107A400004228</td>\n",
       "      <td>CK SP  CK SP33131ORANGE nan ORANGE 4228 nan nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2503</td>\n",
       "      <td>PLC, MFR PART NAME : PLC CPU, MFR PART NO. : 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25032A000045001</td>\n",
       "      <td>FUSE, SHORT NAME  : HCR FUSE 24KV/100A, VOLTAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25032A000045002</td>\n",
       "      <td>SENSOR S30B-2011DA#1026822 สถานที่ใช้งาน TRANS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177924</th>\n",
       "      <td>W410WT140100001</td>\n",
       "      <td>KRAFT, TYPE  : LINER, MFR MODEL NO.  : WT140, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177925</th>\n",
       "      <td>W410WT160000001</td>\n",
       "      <td>KRAFT, TYPE  : LINER, MFR MODEL NO.  : WT160, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177926</th>\n",
       "      <td>W410YL140130001</td>\n",
       "      <td>KRAFT LINER SDY TL140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177927</th>\n",
       "      <td>W431FORMLW00001</td>\n",
       "      <td>แบบฟอร์มกระดาษต่อเนื่อง ใบกำกับภาษี/ใบส่งของ ก...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177928</th>\n",
       "      <td>่J71000215000001</td>\n",
       "      <td>กาว PANATEX P-558H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177929 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Material                                               Text\n",
       "0         210STKR0000018  STK.PE ใสหลังกลาสซีน กาวธรรมดา 180mmx1000m #BW...\n",
       "1       225107A400004228    CK SP  CK SP33131ORANGE nan ORANGE 4228 nan nan\n",
       "2                   2503  PLC, MFR PART NAME : PLC CPU, MFR PART NO. : 6...\n",
       "3        25032A000045001  FUSE, SHORT NAME  : HCR FUSE 24KV/100A, VOLTAG...\n",
       "4        25032A000045002  SENSOR S30B-2011DA#1026822 สถานที่ใช้งาน TRANS...\n",
       "...                  ...                                                ...\n",
       "177924   W410WT140100001  KRAFT, TYPE  : LINER, MFR MODEL NO.  : WT140, ...\n",
       "177925   W410WT160000001  KRAFT, TYPE  : LINER, MFR MODEL NO.  : WT160, ...\n",
       "177926   W410YL140130001                              KRAFT LINER SDY TL140\n",
       "177927   W431FORMLW00001  แบบฟอร์มกระดาษต่อเนื่อง ใบกำกับภาษี/ใบส่งของ ก...\n",
       "177928  ่J71000215000001                                 กาว PANATEX P-558H\n",
       "\n",
       "[177929 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# รวมข้อความตาม Material\n",
    "po_combine_text = (\n",
    "    po.sort_values([\"Material\", \"Line\"])                # เรียงตาม Material และ Line\n",
    "      .groupby(\"Material\")                              # จัดกลุ่มตาม Material\n",
    "      .agg({\"Text\": lambda x: \" \".join(x.astype(str))}) # รวม Text โดยคั่นด้วย space\n",
    "      .reset_index()                                     # ให้ Material กลับมาเป็น column\n",
    ")\n",
    "\n",
    "po_combine_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6648c88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d0/9zz00l_j59ggjlmw8r4v0pn00000gn/T/ipykernel_12970/1059879351.py:1: DtypeWarning: Columns (4,11,14,15,16,17,18,19,20,26,28,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mm = pd.read_csv(\"Vw_MaterialMaster.csv\", encoding=\"utf-8\")\n"
     ]
    }
   ],
   "source": [
    "mm = pd.read_csv(\"Vw_MaterialMaster.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fcc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "material_type = [\"06\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d4b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_mm_matType = mm[\n",
    "    mm[\"Material_Type\"]\n",
    "    .fillna(-1)\n",
    "    .astype(int)\n",
    "    .map(\"{:02d}\".format)\n",
    "    .isin(material_type)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a4fc432",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm[\"pk_plant_matnum\"] = (\n",
    "    mm[\"Plant\"].fillna(\"\").astype(str).str.strip() + \"_\" +\n",
    "    mm[\"Material_Number\"].fillna(\"\").astype(str).str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8113dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_mm_matType = mm[mm[\"Material_Type\"] == 6.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea14659",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_mm_matType_no_dup = select_mm_matType.drop_duplicates(subset=[\"pk_plant_matnum\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5281df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_mm_matType_join = select_mm_matType_no_dup.merge(\n",
    "    po_combine_text,\n",
    "    how=\"left\",\n",
    "    left_on=\"Material_Number\",\n",
    "    right_on=\"Material\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d83e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_mm_matType_mapped = select_mm_matType_join[select_mm_matType_join[\"Material\"].notna()]\n",
    "select_select_mm_matType = select_mm_matType_mapped[[\"pk_plant_matnum\", \"Material_Type\", \"Plant\", \"Material_Number\", \"PlantID\", \"Plant_Description\", \"UOM\", \"Material\", \"Material_Description\", \"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f27010",
   "metadata": {},
   "source": [
    "## 0. Combine field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c9a63",
   "metadata": {},
   "source": [
    "### 0.1 for raw_combine_for_embedding (without uom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296deaf6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43225247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d0/9zz00l_j59ggjlmw8r4v0pn00000gn/T/ipykernel_12970/679890056.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  select_select_mm_matType[\"raw_combine_for_embedding\"] = select_select_mm_matType[\"Material_Description\"] + \" \" + select_select_mm_matType[\"Text\"]\n"
     ]
    }
   ],
   "source": [
    "select_select_mm_matType[\"raw_combine_for_embedding\"] = select_select_mm_matType[\"Material_Description\"] + \" \" + select_select_mm_matType[\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7009c48",
   "metadata": {},
   "source": [
    "### 0.2 for raw_combine_for_keyword (add uom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70dd1f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d0/9zz00l_j59ggjlmw8r4v0pn00000gn/T/ipykernel_12970/4033350750.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  select_select_mm_matType[\"raw_combine_for_keyword\"] = select_select_mm_matType[\"Material_Description\"] + \" \" + select_select_mm_matType[\"Text\"] + \" \" + select_select_mm_matType[\"UOM\"]\n"
     ]
    }
   ],
   "source": [
    "select_select_mm_matType[\"raw_combine_for_keyword\"] = select_select_mm_matType[\"Material_Description\"] + \" \" + select_select_mm_matType[\"Text\"] + \" \" + select_select_mm_matType[\"UOM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f5428",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning & Normalization\n",
    "สรุป: ทำให้ข้อมูลสะอาด สม่ำเสมอ พร้อมให้ embedding และ BM25 ใช้ได้ดี\n",
    "- Uppercase ทั้งหมด\n",
    "- Normalize whitespace\n",
    "- เก็บ character สำคัญของ part code (- / _ . ( ))\n",
    "- Remove noise tokens (EA, PCS, ITEM, DESC)\n",
    "- รวม material_description + PO_text → context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5080e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(text: str) -> str:\n",
    "    # แทน \\n \\t ด้วย space และลด space ซ้อน\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_duplicate_commas_periods(text: str) -> str:\n",
    "    # ลด ,, → , และ .. → .\n",
    "    text = re.sub(r\",{2,}\", \",\", text)\n",
    "    text = re.sub(r\"\\.{2,}\", \".\", text)\n",
    "    return text\n",
    "\n",
    "def lowercase_text(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# --------- Main Cleaning Function ---------\n",
    "def cleaning_features(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. normalize whitespace\n",
    "    text = normalize_whitespace(text)\n",
    "\n",
    "    # 2. remove duplicate punctuation\n",
    "    text = remove_duplicate_commas_periods(text)\n",
    "\n",
    "    # 3. keep punctuation that conveys meaning → (ไม่มีการลบใด ๆ)\n",
    "\n",
    "    # 4. lowercase for embedding stability\n",
    "    # text = lowercase_text(text)\n",
    "\n",
    "    # 5. Normalize dash/underscore variants\n",
    "    text = normalize_dashes(text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_dashes(text):\n",
    "    return text.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "\n",
    "def extract_partcodes(text):\n",
    "    \"\"\"Extract part codes/model numbers from text\"\"\"\n",
    "    # Pattern 1: Alphanumeric with separators (e.g., 3310-ATN9, NU-318)\n",
    "    part_pattern = r\"(?=[A-Z0-9]*[A-Z])[A-Z0-9]+(?:[-_/\\.][A-Z0-9]+)+\"\n",
    "    parts = re.findall(part_pattern, text.upper())\n",
    "    \n",
    "    # Pattern 2: Model numbers (e.g., 3310, 7306)\n",
    "    model_pattern = r'\\b[A-Z0-9]{3,}\\b'\n",
    "    models = re.findall(model_pattern, text.upper())\n",
    "    \n",
    "    # Combine and deduplicate\n",
    "    all_codes = parts + models\n",
    "    return list(dict.fromkeys(all_codes))\n",
    "\n",
    "def normalize_comma_spacing(text: str) -> str:\n",
    "    # บังคับให้มี space หน้าและหลัง \",\" เสมอ\n",
    "    return re.sub(r\"\\s*,\\s*\", \" , \", text).strip()\n",
    "\n",
    "# --------- Build Embedding Text ---------\n",
    "def build_raw_combine_for_embedding(row):\n",
    "    sd = cleaning_features(row[\"Material_Description\"]) if pd.notna(row[\"Material_Description\"]) else \"\"\n",
    "    ld = cleaning_features(row[\"Text\"]) if pd.notna(row[\"Text\"]) else \"\"\n",
    "\n",
    "    combined = f\"{sd} {ld}\"\n",
    "\n",
    "    # # extract part-code to reinforce semantic\n",
    "    # parts = extract_partcodes(combined)\n",
    "    # if parts:\n",
    "    #     combined += \" part_codes: \" + \", \".join(parts)\n",
    "\n",
    "    # normalize comma spacing\n",
    "    combined = normalize_comma_spacing(combined)\n",
    "    \n",
    "    # lowercase everything for embedding stability\n",
    "    combined = lowercase_text(combined)\n",
    "\n",
    "    return combined.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e11391",
   "metadata": {},
   "source": [
    "### 1.1 cleansing for sementic search (embed)\n",
    "- 1. normalize whitespace\n",
    "- 2. remove duplicate punctuation\n",
    "- 3. Normalize dash/underscore variants\n",
    "- 4. extract part-code to reinforce semantic\n",
    "- 5. lowercase everything for embedding stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15d3695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d0/9zz00l_j59ggjlmw8r4v0pn00000gn/T/ipykernel_12970/420435984.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  select_select_mm_matType[\"clean_combine_for_embedding\"] = (\n"
     ]
    }
   ],
   "source": [
    "select_select_mm_matType[\"clean_combine_for_embedding\"] = (\n",
    "    select_select_mm_matType.apply(build_raw_combine_for_embedding, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9423b",
   "metadata": {},
   "source": [
    "### 1.2 cleansing for keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b56d160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function before extracting identifier_keyword\n",
    "def clean_quotes(text: str) -> str:\n",
    "    \"\"\"\n",
    "    - remove \"word\" ที่ไม่มีเลขด้านใน\n",
    "    - remove stray quotes เช่น \"zn → zn\n",
    "    - แปลง inch: 3/4\" → 3/4 inch\n",
    "    - (สุดท้ายค่อย normalize spacing)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # A) remove quoted words without digits → remove entirely\n",
    "    # \"SKF\" → space\n",
    "    # \"MAHLE\" → space\n",
    "    # \"abc-def\" → space\n",
    "    # ---------------------------------------------------------\n",
    "    def remove_non_numeric_quotes(m):\n",
    "        inside = m.group(1)\n",
    "        if not re.search(r'\\d', inside):\n",
    "            return \" \"     # ไม่มีเลข → remove\n",
    "        return m.group(0)  # มีเลข → keep\n",
    "\n",
    "    text = re.sub(r'\"([^\"]*)\"', remove_non_numeric_quotes, text)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # B) remove stray quotes (quote เดี่ยว ๆ ที่เหลือ)\n",
    "    # เช่น \"zn → zn,  zn\" → zn\n",
    "    # ---------------------------------------------------------\n",
    "    text = re.sub(r'(^| )\"(\\w+)', r'\\1\\2', text)\n",
    "    text = re.sub(r'(\\w+)\"( |$)', r'\\1\\2', text)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # C) convert inch (only numeric + double-quote)\n",
    "    # ทำหลังจาก clean quotes แล้ว\n",
    "    # ---------------------------------------------------------\n",
    "    text = re.sub(r'(\\d+/\\d+|\\d+(?:\\.\\d+)?)\"', r'\\1 inch', text)\n",
    "    text = re.sub(r'(\\d+/\\d+|\\d+(?:\\.\\d+)?)\\s*\"(\\s|$)', r'\\1 inch ', text)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # D) normalize spaces\n",
    "    # ---------------------------------------------------------\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "special_chars = [\"?\"]\n",
    "\n",
    "def clean_special_chars(text: str, special_chars: list) -> str:\n",
    "    \"\"\"\n",
    "    Remove / replace special characters defined in special_chars list.\n",
    "    Replace with space to preserve token boundaries.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # escape chars for regex safety (เช่น ?, *, ., etc.)\n",
    "    escaped = [re.escape(c) for c in special_chars]\n",
    "    pattern = f\"[{''.join(escaped)}]\"\n",
    "\n",
    "    # replace with space\n",
    "    text = re.sub(pattern, \" \", text)\n",
    "\n",
    "    # collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_spacing(text):\n",
    "    \"\"\"\n",
    "    - ใส่ space ก่อน–หลัง comma\n",
    "    - compress multiple spaces\n",
    "    \"\"\"\n",
    "\n",
    "    # Add space around commas\n",
    "    text = re.sub(r'\\s*,\\s*', ' , ', text)\n",
    "\n",
    "    # collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_uom(text: str) -> str:\n",
    "\n",
    "    # Convert number + optional space + \" → number inch\n",
    "    # Examples:\n",
    "    #   1/2\"  → 1/2 inch\n",
    "    #   1/2 \" → 1/2 inch\n",
    "    #   10\"   → 10 inch\n",
    "    #   10 \"  → 10 inch\n",
    "    text = re.sub(r'\\b(\\d+(?:/\\d+)?)(\\s*)\"(?![a-zA-Z])', r'\\1 inch', text)\n",
    "\n",
    "    # --- 2. millimeter → mm ---\n",
    "    text = re.sub(r'\\bmil{0,1}imet(er|re|ers|res)\\b', 'mm', text)\n",
    "\n",
    "    # --- 3. kg variations → kg ---\n",
    "    text = re.sub(r'\\bkg\\.?\\b', 'kg', text)\n",
    "    text = re.sub(r'\\bkgs?\\.?\\b', 'kg', text)\n",
    "    text = re.sub(r'\\bkilogram(s)?\\b', 'kg', text)\n",
    "\n",
    "    # --- 4. bars → bar ---\n",
    "    text = re.sub(r'\\bbars\\b', 'bar', text)\n",
    "\n",
    "    # --- 5. centimeter → cm ---\n",
    "    text = re.sub(r'\\bcentimet(er|re|ers|res)\\b', 'cm', text)\n",
    "    text = re.sub(r'\\bcentimeter(s)?\\b', 'cm', text)\n",
    "\n",
    "    # --- 6. meter → m ---  \n",
    "    # (ไม่แตะกรณี M560475D1 เพราะต้องเป็น word boundary)\n",
    "    text = re.sub(r'\\bmet(er|re|ers|res)\\b', 'm', text)\n",
    "    text = re.sub(r'\\bmeters\\b', 'm', text)\n",
    "\n",
    "    # --- 7. liter → L ---\n",
    "    text = re.sub(r'\\blitre(s)?\\b', 'L', text)\n",
    "    text = re.sub(r'\\bliter(s)?\\b', 'L', text)\n",
    "\n",
    "    # --- 8. ton / tonne ---\n",
    "    text = re.sub(r'\\btonne(s)?\\b', 'ton', text)\n",
    "    text = re.sub(r'\\btons\\b', 'ton', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# ============================================================\n",
    "# 2) Preprocess text (normalize punctuation / spacing / dash)\n",
    "# ============================================================\n",
    "def normalize_text(t: str) -> str:\n",
    "    t = t.lower()\n",
    "\n",
    "    # normalize dash variations\n",
    "    t = t.replace(\"–\", \"-\").replace(\"—\", \"-\").replace(\"_\", \"-\")\n",
    "\n",
    "    # remove duplicate punctuations (,,, ---> ,)\n",
    "    # reduce duplicate punctuation (safe)\n",
    "    t = re.sub(r'([!?,.;]{2,})', lambda m: m.group()[0], t)\n",
    "\n",
    "    # # remove unwanted punctuations: * and :\n",
    "    t = t.replace(\"*\", \" \")\n",
    "    t = t.replace(\":\", \" \")\n",
    "\n",
    "    # # normalize spaces\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    return t\n",
    "\n",
    "def normalize_temperature(text):\n",
    "    # -------- Celsius --------\n",
    "    # ครอบคลุม:\n",
    "    # DEGREE CELSIUS, DEG CELSISU, DEG C., \n",
    "    #  C, DEG.CELSIUS,\n",
    "    # DEGREECELSIUS, DEGREE OF CELSIUS, DEGREE OF C, DEG C.\n",
    "    celsius_patterns = [\n",
    "        r'\\b(\\d+)\\s*(deg\\.?|degree\\.?|degrees\\.?)\\s*(of\\s*)?(celsius|celcius|celsisu|cels|cel|c)\\b',\n",
    "        r'\\b(\\d+)\\s*(°)\\s*c\\b',\n",
    "        r'\\b(\\d+)\\s*c\\b'\n",
    "    ]\n",
    "\n",
    "    for p in celsius_patterns:\n",
    "        text = re.sub(p, r'\\1°C', text, flags=re.I)\n",
    "\n",
    "    # -------- Fahrenheit --------\n",
    "    fahrenheit_patterns = [\n",
    "        r'\\b(\\d+)\\s*(deg\\.?|degree\\.?|degrees\\.?)\\s*(of\\s*)?(fahrenheit|farenheit|farenhiet|f)\\b',\n",
    "        r'\\b(\\d+)\\s*(°)\\s*f\\b',\n",
    "        r'\\b(\\d+)\\s*f\\b'\n",
    "    ]\n",
    "\n",
    "    for p in fahrenheit_patterns:\n",
    "        text = re.sub(p, r'\\1°F', text, flags=re.I)\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_technical_terms(text: str) -> str:\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 1) handle dia. followed by number → insert space only\n",
    "    # ------------------------------------------------------------\n",
    "    text = re.sub(r'\\bdia\\.(?=\\d)', r'dia. ', text, flags=re.I)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 2) normalize exact tokens only → dia.\n",
    "    #   - diameter\n",
    "    #   - diam.\n",
    "    #   - dia\n",
    "    #   - dia.  (เฉพาะกรณีเป็น token เดี่ยว ๆ)\n",
    "    #   - ø φ Ø Φ\n",
    "    # ------------------------------------------------------------\n",
    "    # normalize diameter\n",
    "    text = re.sub(r'\\bdiameter\\b', 'dia.', text, flags=re.I)\n",
    "\n",
    "    # normalize diam.\n",
    "    text = re.sub(r'\\bdiam\\.\\b', 'dia.', text, flags=re.I)\n",
    "\n",
    "    # normalize symbol forms\n",
    "    text = re.sub(r'[ØøφΦ⌀]', 'dia.', text)\n",
    "\n",
    "    # normalize dia. (แต่ต้องเป็น token เดี่ยว ไม่ติดตัวเลข)\n",
    "    text = re.sub(r'\\bdia\\.(?!\\d)', 'dia.', text, flags=re.I)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 3) collapse spaces\n",
    "    # ------------------------------------------------------------\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_dash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace dash-like characters with space.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 1) normalize dash variants to space\n",
    "    #   - hyphen-minus (-)\n",
    "    #   - en dash (–)\n",
    "    #   - em dash (—)\n",
    "    # ------------------------------------------------------------\n",
    "    text = re.sub(r'[-–—]', ' ', text)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 2) collapse spaces\n",
    "    # ------------------------------------------------------------\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_slash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace slash characters with space.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 1) normalize slash variants to space\n",
    "    #   - forward slash (/)\n",
    "    #   - backslash (\\)\n",
    "    # ------------------------------------------------------------\n",
    "    text = re.sub(r'[\\\\/]', ' ', text)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 2) collapse spaces\n",
    "    # ------------------------------------------------------------\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a28d99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp for extract_identifier_candidates\n",
    "# ============================================================\n",
    "# 1) Utility regex\n",
    "# ============================================================\n",
    "\n",
    "# แนวๆ CODE/PART NUMBER:\n",
    "code_patterns = [\n",
    "    r\"[A-Z]{1,4}\\d{1,6}[A-Z0-9\\-]*\",    # ABC1234 / E70340M-K059521\n",
    "    r\"\\d{2,4}[A-Z]{1,4}\\d{2,6}\",        # 512ABC1234\n",
    "    r\"[A-Z]{1,4}\\d{2,4}\",               # TLFH-4CA (ก่อน normalize)\n",
    "    r\"\\d{2,4}[A-Z]{1,4}\",               \n",
    "    # r\"\\bAH[- ]?\\d{3,5}\\b\",              # AH24034\n",
    "]\n",
    "\n",
    "code_regex = re.compile(\"|\".join(code_patterns), re.IGNORECASE)\n",
    "# ============================================================\n",
    "# 4) Extract CODE patterns for exact-match boosting\n",
    "# ============================================================\n",
    "def extract_codes(t: str):\n",
    "    return list(set(code_regex.findall(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cf4ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract identifier keyword\n",
    "# 1. extract_identifier_candidates\n",
    "\n",
    "IDENTIFIER_PATTERNS = [\n",
    "\n",
    "    # 1) Classic material / part code\n",
    "    r\"\\b[A-Z]{1,4}[- ]?\\d{2,6}[A-Z0-9]*\\b\",\n",
    "\n",
    "    # 2) Complex code with separators\n",
    "    r\"\\b[A-Z0-9]{2,}[-/][A-Z0-9-/]{2,}\\b\",\n",
    "\n",
    "    # 3) Drawing / model no.\n",
    "    r\"\\b[A-Z]{1,3}\\d{4,}[A-Z0-9/.-]*\\b\",\n",
    "\n",
    "    # 4) Metric thread / fastener\n",
    "    r\"\\bM\\d{2,3}\\s*[xX]\\s*L?\\d{2,4}\\b\",\n",
    "\n",
    "    # 5) Dimension compound\n",
    "    r\"\\b\\d{2,4}\\s*[xX]\\s*\\d{2,5}\\s*(mm|cm|m)?\\b\",\n",
    "\n",
    "    # 6) Pipe / standard\n",
    "    r\"\\bDN\\d{2,4}\\b\",\n",
    "]\n",
    "\n",
    "IDENTIFIER_REGEX = re.compile(\"|\".join(IDENTIFIER_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "\n",
    "def extract_identifier_candidates(text: str) -> list[str]:\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return list(set(m.group(0) for m in IDENTIFIER_REGEX.finditer(text)))\n",
    "\n",
    "\n",
    "# 2. normalize_identifier\n",
    "def normalize_identifier(code: str) -> str:\n",
    "    c = code.strip().upper()\n",
    "\n",
    "    # 1) normalize spaces around separators\n",
    "    c = re.sub(r\"\\s*-\\s*\", \"-\", c)\n",
    "    c = re.sub(r\"\\s*/\\s*\", \"/\", c)\n",
    "    c = re.sub(r\"\\s*[xX]\\s*\", \"x\", c)\n",
    "\n",
    "    # 2) AH 24034 → AH-24034\n",
    "    c = re.sub(r\"\\b([A-Z]{1,4})\\s+(\\d{2,6})\\b\", r\"\\1-\\2\", c)\n",
    "\n",
    "    # 3) normalize unit casing\n",
    "    c = re.sub(r\"\\bMM\\b\", \"MM\", c)\n",
    "\n",
    "    return c\n",
    "\n",
    "# 3. filter_identifier\n",
    "def is_valid_identifier(code: str) -> bool:\n",
    "\n",
    "    # too short\n",
    "    if len(code) < 4:\n",
    "        return False\n",
    "\n",
    "    # pure number\n",
    "    if code.isdigit():\n",
    "        return False\n",
    "\n",
    "    # single dimension like 10MM\n",
    "    if re.fullmatch(r\"\\d{1,4}(MM|CM|M)?\", code):\n",
    "        return False\n",
    "\n",
    "    # must contain at least one digit + one letter\n",
    "    if not (re.search(r\"[A-Z]\", code) and re.search(r\"\\d\", code)):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def extract_identifier_keywords(text: str) -> list[str]:\n",
    "    raw_candidates = extract_identifier_candidates(text)\n",
    "\n",
    "    normalized = [normalize_identifier(c) for c in raw_candidates]\n",
    "\n",
    "    filtered = [c for c in normalized if is_valid_identifier(c)]\n",
    "\n",
    "    return sorted(set(filtered))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e1a8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no(text):\n",
    "    \"\"\"\n",
    "    remove only stopword: \"no.\"\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t.lower() not in [\"no.\"]]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ============================================================\n",
    "# 5) Remove stopwords for BM25 keyword clarity\n",
    "# ============================================================\n",
    "# def remove_stopwords(t: str) -> str:\n",
    "#     tokens = t.split()\n",
    "#     tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "#     return \" \".join(tokens)\n",
    "stopwords_list = [\"short\",\n",
    "    \"name\",\n",
    "    \"long\",\n",
    "    \"description\",\n",
    "    \"remark\",\n",
    "    \"application\",\n",
    "    \"equipment\",\n",
    "    \"machine\",\n",
    "    \"mfr\",\n",
    "    \"maker\",\n",
    "    \"model\",\n",
    "    \"type\",\n",
    "    \"serial\",\n",
    "    \"supplier\",\n",
    "    \"shop\",\n",
    "    \"drawing\",\n",
    "    \"dwg.\",\n",
    "    \"dwg\",\n",
    "    \"no.\",\n",
    "    \"number\",\n",
    "    \"offer\",\n",
    "    \"item\",\n",
    "    \"position\",\n",
    "    \"use for\",\n",
    "    \"used for\"\n",
    "    \"used\",\n",
    "    \"?\",\n",
    "    \"from\",\n",
    "    \"part\",\n",
    "    \"partno\",\n",
    "    \"code\",\n",
    "    \"unit\",\n",
    "    \"qty.\",\n",
    "    \"qty\",\n",
    "    \"size\",\n",
    "    \"pressure\",\n",
    "    \"range\"\n",
    "]\n",
    "\n",
    "stopwords = set(stopwords_list)\n",
    "\n",
    "def remove_stopwords(t: str) -> str:\n",
    "    tokens = t.split()\n",
    "    cleaned = []\n",
    "    for tok in tokens:\n",
    "        tok_clean = tok.strip(\",:;\")  # ลบ punctuation รอบ ๆ\n",
    "        if tok_clean and tok_clean not in stopwords:  # <--- ป้องกัน \"\" \n",
    "            cleaned.append(tok_clean)\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "# ============================================================\n",
    "# 6) Create keyword_list field\n",
    "# ============================================================\n",
    "def build_keyword_list(clean_text: str, codes: list):\n",
    "    tokens = clean_text.split()\n",
    "    return list(sorted(set(tokens + codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13b76d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d0/9zz00l_j59ggjlmw8r4v0pn00000gn/T/ipykernel_12970/3804627580.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  select_select_mm_matType[\"clean_combine_for_keyword\"], select_select_mm_matType[\"keyword_codes\"], select_select_mm_matType[\"keyword_old_codes\"], select_select_mm_matType[\"keyword_list\"] = zip(\n",
      "/var/folders/d0/9zz00l_j59ggjlmw8r4v0pn00000gn/T/ipykernel_12970/3804627580.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  select_select_mm_matType[\"clean_combine_for_keyword\"], select_select_mm_matType[\"keyword_codes\"], select_select_mm_matType[\"keyword_old_codes\"], select_select_mm_matType[\"keyword_list\"] = zip(\n",
      "/var/folders/d0/9zz00l_j59ggjlmw8r4v0pn00000gn/T/ipykernel_12970/3804627580.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  select_select_mm_matType[\"clean_combine_for_keyword\"], select_select_mm_matType[\"keyword_codes\"], select_select_mm_matType[\"keyword_old_codes\"], select_select_mm_matType[\"keyword_list\"] = zip(\n",
      "/var/folders/d0/9zz00l_j59ggjlmw8r4v0pn00000gn/T/ipykernel_12970/3804627580.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  select_select_mm_matType[\"clean_combine_for_keyword\"], select_select_mm_matType[\"keyword_codes\"], select_select_mm_matType[\"keyword_old_codes\"], select_select_mm_matType[\"keyword_list\"] = zip(\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pk_plant_matnum",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Material_Type",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Plant",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Material_Number",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "PlantID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Plant_Description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "UOM",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Material",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Material_Description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "raw_combine_for_embedding",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "raw_combine_for_keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_combine_for_embedding",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_combine_for_keyword",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "keyword_codes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "keyword_old_codes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "keyword_list",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "887829a2-af70-4012-9e19-0f134325db82",
       "rows": [
        [
         "0",
         "7560_7506809480106",
         "6.0",
         "7560",
         "7506809480106",
         "PT011",
         "SKIC-Wangsa-la",
         "PC",
         "7506809480106",
         "SPROCKET,CHAIN,49 T ,1/2 \"(ZN)",
         "SPROCKET,CHAIN, SHORT NAME  : SPROCKET,CHAIN, NO. OF STRANDS  : 1, NO. OF TEETH  : 49, FOR CHAIN NO.  : ?, PITCH  : 1/2 \", MATERIAL  : CARBON STEEL, APPLICATION  : SILENT CHAIN STOKER PB 8, REMARK  : SPROCKET DRIVEN MAT. MEDIUM CARBON STEEL TOOTH HARD SURFACE NO. OF TEETH 49, PITCH 0.500\" AS PER DWG.NO. 106 309 B 001 FOR SILENT CHAIN STOKER PB 8",
         "SPROCKET,CHAIN,49 T ,1/2 \"(ZN) SPROCKET,CHAIN, SHORT NAME  : SPROCKET,CHAIN, NO. OF STRANDS  : 1, NO. OF TEETH  : 49, FOR CHAIN NO.  : ?, PITCH  : 1/2 \", MATERIAL  : CARBON STEEL, APPLICATION  : SILENT CHAIN STOKER PB 8, REMARK  : SPROCKET DRIVEN MAT. MEDIUM CARBON STEEL TOOTH HARD SURFACE NO. OF TEETH 49, PITCH 0.500\" AS PER DWG.NO. 106 309 B 001 FOR SILENT CHAIN STOKER PB 8",
         "SPROCKET,CHAIN,49 T ,1/2 \"(ZN) SPROCKET,CHAIN, SHORT NAME  : SPROCKET,CHAIN, NO. OF STRANDS  : 1, NO. OF TEETH  : 49, FOR CHAIN NO.  : ?, PITCH  : 1/2 \", MATERIAL  : CARBON STEEL, APPLICATION  : SILENT CHAIN STOKER PB 8, REMARK  : SPROCKET DRIVEN MAT. MEDIUM CARBON STEEL TOOTH HARD SURFACE NO. OF TEETH 49, PITCH 0.500\" AS PER DWG.NO. 106 309 B 001 FOR SILENT CHAIN STOKER PB 8 PC",
         "sprocket , chain , 49 t , 1/2 \"(zn) sprocket , chain , short name : sprocket , chain , no. of strands : 1 , no. of teeth : 49 , for chain no. : ? , pitch : 1/2 \" , material : carbon steel , application : silent chain stoker pb 8 , remark : sprocket driven mat. medium carbon steel tooth hard surface no. of teeth 49 , pitch 0.500\" as per dwg.no. 106 309 b 001 for silent chain stoker pb 8",
         "sprocket , chain , 49 t , 1/2 inch(zn) sprocket , chain , short name sprocket , chain , of strands 1 , of teeth 49 , for chain , pitch 1/2 inch , material carbon steel , application silent chain stoker pb 8 , remark sprocket driven mat. medium carbon steel tooth hard surface of teeth 49 , pitch 0.500 as per dwg.no. 106 309 b 001 for silent chain stoker pb 8 pc",
         "['B-001']",
         "[]",
         "['0.500', '001', '1', '1/2', '106', '309', '49', '8', 'B-001', 'as', 'b', 'carbon', 'chain', 'driven', 'dwg.no.', 'for', 'hard', 'inch', 'inch(zn)', 'mat.', 'material', 'medium', 'of', 'pb', 'pc', 'per', 'pitch', 'silent', 'sprocket', 'steel', 'stoker', 'strands', 'surface', 't', 'teeth', 'tooth']"
        ],
        [
         "1",
         "7560_7806055050934",
         "6.0",
         "7560",
         "7806055050934",
         "PT011",
         "SKIC-Wangsa-la",
         "PC",
         "7806055050934",
         "FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA",
         "FLOWMETER, SHORT NAME  : FLOWMETER, FLOW RANGE  : 300L/MIN, MFR MODEL NO.  : TLFH-4CA, MFR NAME  : KYTOLA, REMARK  : FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA",
         "FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA FLOWMETER, SHORT NAME  : FLOWMETER, FLOW RANGE  : 300L/MIN, MFR MODEL NO.  : TLFH-4CA, MFR NAME  : KYTOLA, REMARK  : FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA",
         "FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA FLOWMETER, SHORT NAME  : FLOWMETER, FLOW RANGE  : 300L/MIN, MFR MODEL NO.  : TLFH-4CA, MFR NAME  : KYTOLA, REMARK  : FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA PC",
         "flowmeter , 300l/min , tlfh-4ca , kytola flowmeter , short name : flowmeter , flow range : 300l/min , mfr model no. : tlfh-4ca , mfr name : kytola , remark : flowmeter , 300l/min , tlfh-4ca , kytola",
         "flowmeter , 300l/min , tlfh-4ca , kytola flowmeter , short name flowmeter , flow range 300l/min , mfr model tlfh-4ca , mfr name kytola , remark flowmeter , 300l/min , tlfh-4ca , kytola pc",
         "['300L/MIN', 'TLFH-4CA']",
         "['300l']",
         "['300L/MIN', '300l/min', 'TLFH-4CA', 'flow', 'flowmeter', 'kytola', 'pc', 'tlfh-4ca']"
        ],
        [
         "2",
         "7560_54060106035573",
         "6.0",
         "7560",
         "54060106035573",
         "PT011",
         "SKIC-Wangsa-la",
         "PC",
         "54060106035573",
         "ACID PHOSPHORIC COMERCIAL GRADE",
         "ACID PHOSPHORIC COMERCIAL GRADE, MACHINE CODE  : WASHER,E70340M-K059521,E70540M-L014006,E70540M-L014001                 E70540M-L014005,E70540M-L014003,E70540M-L014004,E70540M-L014008,E70340M-K059249,E70340M-K059088,E70340M-K059001                 E70540M-L014007, EQUIPMENT TYPE  : FIBER LINE FOR BROWN STOCK WASHER, MFR NAME  : DORR-OLIVER, MFR MODEL NO.  : MODEL 512, MFR PART NO.  : ?, REMARK  : ACID PHOSPHORIC COMERCIAL GRADE (กรดบัดกรี) 35 KGS.",
         "ACID PHOSPHORIC COMERCIAL GRADE ACID PHOSPHORIC COMERCIAL GRADE, MACHINE CODE  : WASHER,E70340M-K059521,E70540M-L014006,E70540M-L014001                 E70540M-L014005,E70540M-L014003,E70540M-L014004,E70540M-L014008,E70340M-K059249,E70340M-K059088,E70340M-K059001                 E70540M-L014007, EQUIPMENT TYPE  : FIBER LINE FOR BROWN STOCK WASHER, MFR NAME  : DORR-OLIVER, MFR MODEL NO.  : MODEL 512, MFR PART NO.  : ?, REMARK  : ACID PHOSPHORIC COMERCIAL GRADE (กรดบัดกรี) 35 KGS.",
         "ACID PHOSPHORIC COMERCIAL GRADE ACID PHOSPHORIC COMERCIAL GRADE, MACHINE CODE  : WASHER,E70340M-K059521,E70540M-L014006,E70540M-L014001                 E70540M-L014005,E70540M-L014003,E70540M-L014004,E70540M-L014008,E70340M-K059249,E70340M-K059088,E70340M-K059001                 E70540M-L014007, EQUIPMENT TYPE  : FIBER LINE FOR BROWN STOCK WASHER, MFR NAME  : DORR-OLIVER, MFR MODEL NO.  : MODEL 512, MFR PART NO.  : ?, REMARK  : ACID PHOSPHORIC COMERCIAL GRADE (กรดบัดกรี) 35 KGS. PC",
         "acid phosphoric comercial grade acid phosphoric comercial grade , machine code : washer , e70340m-k059521 , e70540m-l014006 , e70540m-l014001 e70540m-l014005 , e70540m-l014003 , e70540m-l014004 , e70540m-l014008 , e70340m-k059249 , e70340m-k059088 , e70340m-k059001 e70540m-l014007 , equipment type : fiber line for brown stock washer , mfr name : dorr-oliver , mfr model no. : model 512 , mfr part no. : ? , remark : acid phosphoric comercial grade (กรดบัดกรี) 35 kgs.",
         "acid phosphoric comercial grade acid phosphoric comercial grade , machine code washer , e70340m-k059521 , e70540m-l014006 , e70540m-l014001 e70540m-l014005 , e70540m-l014003 , e70540m-l014004 , e70540m-l014008 , e70340m-k059249 , e70340m-k059088 , e70340m-k059001 e70540m-l014007 , equipment type fiber line for brown stock washer , mfr name dorr-oliver , mfr model model 512 , mfr part , remark acid phosphoric comercial grade (กรดบัดกรี) 35 kg. pc",
         "['E70340M', 'E70540M', 'K059001', 'K059088', 'K059249', 'K059521', 'L014001', 'L014003', 'L014004', 'L014005', 'L014006', 'L014007', 'L014008']",
         "['e70340m-k059088', 'e70540m-l014006', 'e70540m-l014004', 'e70340m-k059521', 'e70540m-l014007', 'e70540m-l014001', 'e70340m-k059001', 'e70540m-l014008', 'e70540m-l014003', 'e70340m-k059249', 'e70540m-l014005']",
         "['(กรดบัดกรี)', '35', '512', 'E70340M', 'E70540M', 'K059001', 'K059088', 'K059249', 'K059521', 'L014001', 'L014003', 'L014004', 'L014005', 'L014006', 'L014007', 'L014008', 'acid', 'brown', 'comercial', 'dorr-oliver', 'e70340m-k059001', 'e70340m-k059088', 'e70340m-k059249', 'e70340m-k059521', 'e70540m-l014001', 'e70540m-l014003', 'e70540m-l014004', 'e70540m-l014005', 'e70540m-l014006', 'e70540m-l014007', 'e70540m-l014008', 'fiber', 'for', 'grade', 'kg.', 'line', 'pc', 'phosphoric', 'stock', 'washer']"
        ],
        [
         "3",
         "7560_54060140106258",
         "6.0",
         "7560",
         "54060140106258",
         "PT011",
         "SKIC-Wangsa-la",
         "PC",
         "54060140106258",
         "BEARING,ACC,WITHDRAWAL SLEEVE,AH 24034",
         "SHORT NAME  : BEARING,ACC, MFR NAME  : SKF, TYPE  : WITHDRAWAL SLEEVE, MFR PART NO.  : AH 24034, THREAD SIZE  : M 180 X 3, APPLICATION  : DEWATERING PRESS C-TMP#2,  REMARK : ADAPTER SLEEVE WITHDRAWAL AH-24034 \"SKF\" FOR DEWATERING PRESS C-TMP#2",
         "BEARING,ACC,WITHDRAWAL SLEEVE,AH 24034 SHORT NAME  : BEARING,ACC, MFR NAME  : SKF, TYPE  : WITHDRAWAL SLEEVE, MFR PART NO.  : AH 24034, THREAD SIZE  : M 180 X 3, APPLICATION  : DEWATERING PRESS C-TMP#2,  REMARK : ADAPTER SLEEVE WITHDRAWAL AH-24034 \"SKF\" FOR DEWATERING PRESS C-TMP#2",
         "BEARING,ACC,WITHDRAWAL SLEEVE,AH 24034 SHORT NAME  : BEARING,ACC, MFR NAME  : SKF, TYPE  : WITHDRAWAL SLEEVE, MFR PART NO.  : AH 24034, THREAD SIZE  : M 180 X 3, APPLICATION  : DEWATERING PRESS C-TMP#2,  REMARK : ADAPTER SLEEVE WITHDRAWAL AH-24034 \"SKF\" FOR DEWATERING PRESS C-TMP#2 PC",
         "bearing , acc , withdrawal sleeve , ah 24034 short name : bearing , acc , mfr name : skf , type : withdrawal sleeve , mfr part no. : ah 24034 , thread size : m 180 x 3 , application : dewatering press c-tmp#2 , remark : adapter sleeve withdrawal ah-24034 \"skf\" for dewatering press c-tmp#2",
         "bearing , acc , withdrawal sleeve , ah 24034 short name bearing , acc , mfr name skf , type withdrawal sleeve , mfr part ah 24034 , thread size m 180 x 3 , application dewatering press c-tmp#2 , remark adapter sleeve withdrawal ah-24034 for dewatering press c-tmp#2 pc",
         "['AH-24034', 'M-180']",
         "[]",
         "['180', '24034', '3', 'AH-24034', 'M-180', 'acc', 'adapter', 'ah', 'ah-24034', 'bearing', 'c-tmp#2', 'dewatering', 'for', 'm', 'pc', 'press', 'skf', 'sleeve', 'thread', 'withdrawal', 'x']"
        ],
        [
         "4",
         "7560_54060140113243",
         "6.0",
         "7560",
         "54060140113243",
         "PT011",
         "SKIC-Wangsa-la",
         "PC",
         "54060140113243",
         "BEARING,ACC,WITHDRAWAL SLEEVE,AH 24028",
         "BEARING,ACC, MFR NAME  : SKF, TYPE  : WITHDRAWAL SLEEVE, MFR PART NO.  : AH 24028, APPLICATION  : WASHER PRESS NO.1,2, REMARK  : ADAPTER SLEEVE WITHDRAWAL AH 24028 SUPP: SKF",
         "BEARING,ACC,WITHDRAWAL SLEEVE,AH 24028 BEARING,ACC, MFR NAME  : SKF, TYPE  : WITHDRAWAL SLEEVE, MFR PART NO.  : AH 24028, APPLICATION  : WASHER PRESS NO.1,2, REMARK  : ADAPTER SLEEVE WITHDRAWAL AH 24028 SUPP: SKF",
         "BEARING,ACC,WITHDRAWAL SLEEVE,AH 24028 BEARING,ACC, MFR NAME  : SKF, TYPE  : WITHDRAWAL SLEEVE, MFR PART NO.  : AH 24028, APPLICATION  : WASHER PRESS NO.1,2, REMARK  : ADAPTER SLEEVE WITHDRAWAL AH 24028 SUPP: SKF PC",
         "bearing , acc , withdrawal sleeve , ah 24028 bearing , acc , mfr name : skf , type : withdrawal sleeve , mfr part no. : ah 24028 , application : washer press no.1 , 2 , remark : adapter sleeve withdrawal ah 24028 supp: skf",
         "bearing , acc , withdrawal sleeve , ah 24028 bearing , acc , mfr name skf , type withdrawal sleeve , mfr part ah 24028 , application washer press no.1 , 2 , remark adapter sleeve withdrawal ah 24028 supp skf pc",
         "['AH-24028']",
         "[]",
         "['2', '24028', 'AH-24028', 'acc', 'adapter', 'ah', 'bearing', 'no.1', 'pc', 'press', 'skf', 'sleeve', 'supp', 'washer', 'withdrawal']"
        ]
       ],
       "shape": {
        "columns": 17,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pk_plant_matnum</th>\n",
       "      <th>Material_Type</th>\n",
       "      <th>Plant</th>\n",
       "      <th>Material_Number</th>\n",
       "      <th>PlantID</th>\n",
       "      <th>Plant_Description</th>\n",
       "      <th>UOM</th>\n",
       "      <th>Material</th>\n",
       "      <th>Material_Description</th>\n",
       "      <th>Text</th>\n",
       "      <th>raw_combine_for_embedding</th>\n",
       "      <th>raw_combine_for_keyword</th>\n",
       "      <th>clean_combine_for_embedding</th>\n",
       "      <th>clean_combine_for_keyword</th>\n",
       "      <th>keyword_codes</th>\n",
       "      <th>keyword_old_codes</th>\n",
       "      <th>keyword_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7560_7506809480106</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>7506809480106</td>\n",
       "      <td>PT011</td>\n",
       "      <td>SKIC-Wangsa-la</td>\n",
       "      <td>PC</td>\n",
       "      <td>7506809480106</td>\n",
       "      <td>SPROCKET,CHAIN,49 T ,1/2 \"(ZN)</td>\n",
       "      <td>SPROCKET,CHAIN, SHORT NAME  : SPROCKET,CHAIN, ...</td>\n",
       "      <td>SPROCKET,CHAIN,49 T ,1/2 \"(ZN) SPROCKET,CHAIN,...</td>\n",
       "      <td>SPROCKET,CHAIN,49 T ,1/2 \"(ZN) SPROCKET,CHAIN,...</td>\n",
       "      <td>sprocket , chain , 49 t , 1/2 \"(zn) sprocket ,...</td>\n",
       "      <td>sprocket , chain , 49 t , 1/2 inch(zn) sprocke...</td>\n",
       "      <td>[B-001]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.500, 001, 1, 1/2, 106, 309, 49, 8, B-001, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7560_7806055050934</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>7806055050934</td>\n",
       "      <td>PT011</td>\n",
       "      <td>SKIC-Wangsa-la</td>\n",
       "      <td>PC</td>\n",
       "      <td>7806055050934</td>\n",
       "      <td>FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA</td>\n",
       "      <td>FLOWMETER, SHORT NAME  : FLOWMETER, FLOW RANGE...</td>\n",
       "      <td>FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA FLOWMETER, ...</td>\n",
       "      <td>FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA FLOWMETER, ...</td>\n",
       "      <td>flowmeter , 300l/min , tlfh-4ca , kytola flowm...</td>\n",
       "      <td>flowmeter , 300l/min , tlfh-4ca , kytola flowm...</td>\n",
       "      <td>[300L/MIN, TLFH-4CA]</td>\n",
       "      <td>[300l]</td>\n",
       "      <td>[300L/MIN, 300l/min, TLFH-4CA, flow, flowmeter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7560_54060106035573</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>54060106035573</td>\n",
       "      <td>PT011</td>\n",
       "      <td>SKIC-Wangsa-la</td>\n",
       "      <td>PC</td>\n",
       "      <td>54060106035573</td>\n",
       "      <td>ACID PHOSPHORIC COMERCIAL GRADE</td>\n",
       "      <td>ACID PHOSPHORIC COMERCIAL GRADE, MACHINE CODE ...</td>\n",
       "      <td>ACID PHOSPHORIC COMERCIAL GRADE ACID PHOSPHORI...</td>\n",
       "      <td>ACID PHOSPHORIC COMERCIAL GRADE ACID PHOSPHORI...</td>\n",
       "      <td>acid phosphoric comercial grade acid phosphori...</td>\n",
       "      <td>acid phosphoric comercial grade acid phosphori...</td>\n",
       "      <td>[E70340M, E70540M, K059001, K059088, K059249, ...</td>\n",
       "      <td>[e70340m-k059088, e70540m-l014006, e70540m-l01...</td>\n",
       "      <td>[(กรดบัดกรี), 35, 512, E70340M, E70540M, K0590...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7560_54060140106258</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>54060140106258</td>\n",
       "      <td>PT011</td>\n",
       "      <td>SKIC-Wangsa-la</td>\n",
       "      <td>PC</td>\n",
       "      <td>54060140106258</td>\n",
       "      <td>BEARING,ACC,WITHDRAWAL SLEEVE,AH 24034</td>\n",
       "      <td>SHORT NAME  : BEARING,ACC, MFR NAME  : SKF, TY...</td>\n",
       "      <td>BEARING,ACC,WITHDRAWAL SLEEVE,AH 24034 SHORT N...</td>\n",
       "      <td>BEARING,ACC,WITHDRAWAL SLEEVE,AH 24034 SHORT N...</td>\n",
       "      <td>bearing , acc , withdrawal sleeve , ah 24034 s...</td>\n",
       "      <td>bearing , acc , withdrawal sleeve , ah 24034 s...</td>\n",
       "      <td>[AH-24034, M-180]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[180, 24034, 3, AH-24034, M-180, acc, adapter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7560_54060140113243</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>54060140113243</td>\n",
       "      <td>PT011</td>\n",
       "      <td>SKIC-Wangsa-la</td>\n",
       "      <td>PC</td>\n",
       "      <td>54060140113243</td>\n",
       "      <td>BEARING,ACC,WITHDRAWAL SLEEVE,AH 24028</td>\n",
       "      <td>BEARING,ACC, MFR NAME  : SKF, TYPE  : WITHDRAW...</td>\n",
       "      <td>BEARING,ACC,WITHDRAWAL SLEEVE,AH 24028 BEARING...</td>\n",
       "      <td>BEARING,ACC,WITHDRAWAL SLEEVE,AH 24028 BEARING...</td>\n",
       "      <td>bearing , acc , withdrawal sleeve , ah 24028 b...</td>\n",
       "      <td>bearing , acc , withdrawal sleeve , ah 24028 b...</td>\n",
       "      <td>[AH-24028]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2, 24028, AH-24028, acc, adapter, ah, bearing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pk_plant_matnum  Material_Type Plant Material_Number PlantID  \\\n",
       "0   7560_7506809480106            6.0  7560   7506809480106   PT011   \n",
       "1   7560_7806055050934            6.0  7560   7806055050934   PT011   \n",
       "2  7560_54060106035573            6.0  7560  54060106035573   PT011   \n",
       "3  7560_54060140106258            6.0  7560  54060140106258   PT011   \n",
       "4  7560_54060140113243            6.0  7560  54060140113243   PT011   \n",
       "\n",
       "  Plant_Description UOM        Material  \\\n",
       "0    SKIC-Wangsa-la  PC   7506809480106   \n",
       "1    SKIC-Wangsa-la  PC   7806055050934   \n",
       "2    SKIC-Wangsa-la  PC  54060106035573   \n",
       "3    SKIC-Wangsa-la  PC  54060140106258   \n",
       "4    SKIC-Wangsa-la  PC  54060140113243   \n",
       "\n",
       "                     Material_Description  \\\n",
       "0          SPROCKET,CHAIN,49 T ,1/2 \"(ZN)   \n",
       "1      FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA   \n",
       "2         ACID PHOSPHORIC COMERCIAL GRADE   \n",
       "3  BEARING,ACC,WITHDRAWAL SLEEVE,AH 24034   \n",
       "4  BEARING,ACC,WITHDRAWAL SLEEVE,AH 24028   \n",
       "\n",
       "                                                Text  \\\n",
       "0  SPROCKET,CHAIN, SHORT NAME  : SPROCKET,CHAIN, ...   \n",
       "1  FLOWMETER, SHORT NAME  : FLOWMETER, FLOW RANGE...   \n",
       "2  ACID PHOSPHORIC COMERCIAL GRADE, MACHINE CODE ...   \n",
       "3  SHORT NAME  : BEARING,ACC, MFR NAME  : SKF, TY...   \n",
       "4  BEARING,ACC, MFR NAME  : SKF, TYPE  : WITHDRAW...   \n",
       "\n",
       "                           raw_combine_for_embedding  \\\n",
       "0  SPROCKET,CHAIN,49 T ,1/2 \"(ZN) SPROCKET,CHAIN,...   \n",
       "1  FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA FLOWMETER, ...   \n",
       "2  ACID PHOSPHORIC COMERCIAL GRADE ACID PHOSPHORI...   \n",
       "3  BEARING,ACC,WITHDRAWAL SLEEVE,AH 24034 SHORT N...   \n",
       "4  BEARING,ACC,WITHDRAWAL SLEEVE,AH 24028 BEARING...   \n",
       "\n",
       "                             raw_combine_for_keyword  \\\n",
       "0  SPROCKET,CHAIN,49 T ,1/2 \"(ZN) SPROCKET,CHAIN,...   \n",
       "1  FLOWMETER,300L/MIN,TLFH-4CA,KYTOLA FLOWMETER, ...   \n",
       "2  ACID PHOSPHORIC COMERCIAL GRADE ACID PHOSPHORI...   \n",
       "3  BEARING,ACC,WITHDRAWAL SLEEVE,AH 24034 SHORT N...   \n",
       "4  BEARING,ACC,WITHDRAWAL SLEEVE,AH 24028 BEARING...   \n",
       "\n",
       "                         clean_combine_for_embedding  \\\n",
       "0  sprocket , chain , 49 t , 1/2 \"(zn) sprocket ,...   \n",
       "1  flowmeter , 300l/min , tlfh-4ca , kytola flowm...   \n",
       "2  acid phosphoric comercial grade acid phosphori...   \n",
       "3  bearing , acc , withdrawal sleeve , ah 24034 s...   \n",
       "4  bearing , acc , withdrawal sleeve , ah 24028 b...   \n",
       "\n",
       "                           clean_combine_for_keyword  \\\n",
       "0  sprocket , chain , 49 t , 1/2 inch(zn) sprocke...   \n",
       "1  flowmeter , 300l/min , tlfh-4ca , kytola flowm...   \n",
       "2  acid phosphoric comercial grade acid phosphori...   \n",
       "3  bearing , acc , withdrawal sleeve , ah 24034 s...   \n",
       "4  bearing , acc , withdrawal sleeve , ah 24028 b...   \n",
       "\n",
       "                                       keyword_codes  \\\n",
       "0                                            [B-001]   \n",
       "1                               [300L/MIN, TLFH-4CA]   \n",
       "2  [E70340M, E70540M, K059001, K059088, K059249, ...   \n",
       "3                                  [AH-24034, M-180]   \n",
       "4                                         [AH-24028]   \n",
       "\n",
       "                                   keyword_old_codes  \\\n",
       "0                                                 []   \n",
       "1                                             [300l]   \n",
       "2  [e70340m-k059088, e70540m-l014006, e70540m-l01...   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                        keyword_list  \n",
       "0  [0.500, 001, 1, 1/2, 106, 309, 49, 8, B-001, a...  \n",
       "1  [300L/MIN, 300l/min, TLFH-4CA, flow, flowmeter...  \n",
       "2  [(กรดบัดกรี), 35, 512, E70340M, E70540M, K0590...  \n",
       "3  [180, 24034, 3, AH-24034, M-180, acc, adapter,...  \n",
       "4  [2, 24028, AH-24028, acc, adapter, ah, bearing...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Main logic for keyword pipeline\n",
    "# ============================================================\n",
    "def clean_keyword_pipeline(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\", [], [], []\n",
    "\n",
    "    t = text\n",
    "\n",
    "    # 1) special \" cleaner (change \" to inch / garbage / remove)\n",
    "    t = clean_quotes(t)\n",
    "\n",
    "    # 2) clean special chars\n",
    "    t = clean_special_chars(t, special_chars)\n",
    "\n",
    "    # 3) Add space around comma\n",
    "    t = normalize_spacing(t)\n",
    "\n",
    "    # # 4) normalize\n",
    "    t = normalize_text(t)\n",
    "\n",
    "    # # 5) normalize UOM\n",
    "    t = normalize_uom(t)\n",
    "\n",
    "    # # 6) normalize degree celsius and Fahrenheit\n",
    "    t = normalize_temperature(t)\n",
    "\n",
    "    # # 7) normalize technical term\n",
    "    t = normalize_technical_terms(t)\n",
    "\n",
    "    # clean \"-\" and \"/\" for test vector search\n",
    "    # t = clean_dash(t) # Recall drop\n",
    "    # t = clean_slash(t) # Recall drop\n",
    "\n",
    "    # # 8) extract codes\n",
    "    old_codes = extract_codes(t)\n",
    "    codes = extract_identifier_keywords(t)\n",
    "\n",
    "    # # 9) remove no.\n",
    "    t = remove_no(t)\n",
    "\n",
    "    # 10) remove stopwords\n",
    "    t_no_stop = remove_stopwords(t)\n",
    "\n",
    "    # 5) build keyword list\n",
    "    keyword_list = build_keyword_list(t_no_stop, codes)\n",
    "    # codes = \"\"\n",
    "    # keyword_list = \"\"\n",
    "\n",
    "    return t, codes, old_codes, keyword_list\n",
    "\n",
    "\n",
    "\n",
    "select_select_mm_matType[\"clean_combine_for_keyword\"], select_select_mm_matType[\"keyword_codes\"], select_select_mm_matType[\"keyword_old_codes\"], select_select_mm_matType[\"keyword_list\"] = zip(\n",
    "    *select_select_mm_matType[\"raw_combine_for_keyword\"].map(clean_keyword_pipeline)\n",
    ")\n",
    "\n",
    "select_select_mm_matType.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e490d",
   "metadata": {},
   "source": [
    "## Prepare test Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45b2d9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "MODEL_BRAND",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Key_mat_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Plant_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Mat_1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Key_mat_2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Plant_2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Mat_2",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b5ac9e82-a9f4-468a-9f8a-897ee606ff70",
       "rows": [
        [
         "0",
         "3310 ATN9_SKF",
         "7560_75060602310346",
         "7560",
         "75060602310346",
         "7511_75060610300414",
         "7511",
         "75060610300414"
        ],
        [
         "1",
         "3311 ANR_SKF",
         "7560_75060602411346",
         "7560",
         "75060602411346",
         "7511_75060610310502",
         "7511",
         "75060610310502"
        ],
        [
         "2",
         "7306 BECBM_SKF",
         "7812_78060606306581",
         "7812",
         "78060606306581",
         "7511_75060610805312",
         "7511",
         "75060610805312"
        ],
        [
         "3",
         "NU 308 ECP/C3_SKF",
         "7812_78060612308552",
         "7812",
         "78060612308552",
         "7511_75060612500369",
         "7511",
         "75060612500369"
        ],
        [
         "4",
         "C 2234 K/C4_SKF",
         "7560_75060612234350",
         "7560",
         "75060612234350",
         "7511_75060612556307",
         "7511",
         "75060612556307"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL_BRAND</th>\n",
       "      <th>Key_mat_1</th>\n",
       "      <th>Plant_1</th>\n",
       "      <th>Mat_1</th>\n",
       "      <th>Key_mat_2</th>\n",
       "      <th>Plant_2</th>\n",
       "      <th>Mat_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3310 ATN9_SKF</td>\n",
       "      <td>7560_75060602310346</td>\n",
       "      <td>7560</td>\n",
       "      <td>75060602310346</td>\n",
       "      <td>7511_75060610300414</td>\n",
       "      <td>7511</td>\n",
       "      <td>75060610300414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3311 ANR_SKF</td>\n",
       "      <td>7560_75060602411346</td>\n",
       "      <td>7560</td>\n",
       "      <td>75060602411346</td>\n",
       "      <td>7511_75060610310502</td>\n",
       "      <td>7511</td>\n",
       "      <td>75060610310502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7306 BECBM_SKF</td>\n",
       "      <td>7812_78060606306581</td>\n",
       "      <td>7812</td>\n",
       "      <td>78060606306581</td>\n",
       "      <td>7511_75060610805312</td>\n",
       "      <td>7511</td>\n",
       "      <td>75060610805312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NU 308 ECP/C3_SKF</td>\n",
       "      <td>7812_78060612308552</td>\n",
       "      <td>7812</td>\n",
       "      <td>78060612308552</td>\n",
       "      <td>7511_75060612500369</td>\n",
       "      <td>7511</td>\n",
       "      <td>75060612500369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C 2234 K/C4_SKF</td>\n",
       "      <td>7560_75060612234350</td>\n",
       "      <td>7560</td>\n",
       "      <td>75060612234350</td>\n",
       "      <td>7511_75060612556307</td>\n",
       "      <td>7511</td>\n",
       "      <td>75060612556307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MODEL_BRAND            Key_mat_1 Plant_1           Mat_1  \\\n",
       "0      3310 ATN9_SKF  7560_75060602310346    7560  75060602310346   \n",
       "1       3311 ANR_SKF  7560_75060602411346    7560  75060602411346   \n",
       "2     7306 BECBM_SKF  7812_78060606306581    7812  78060606306581   \n",
       "3  NU 308 ECP/C3_SKF  7812_78060612308552    7812  78060612308552   \n",
       "4    C 2234 K/C4_SKF  7560_75060612234350    7560  75060612234350   \n",
       "\n",
       "             Key_mat_2 Plant_2           Mat_2  \n",
       "0  7511_75060610300414    7511  75060610300414  \n",
       "1  7511_75060610310502    7511  75060610310502  \n",
       "2  7511_75060610805312    7511  75060610805312  \n",
       "3  7511_75060612500369    7511  75060612500369  \n",
       "4  7511_75060612556307    7511  75060612556307  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = pd.read_csv(\"edges.csv\")\n",
    "\n",
    "# 1) เอา 50 แถวแรก\n",
    "head50 = edges.head(50)\n",
    "\n",
    "# 2) เอา random 50 แถว (ไม่ซ้ำกับ head50)\n",
    "random50 = edges.iloc[50:].sample(n=50, random_state=42)\n",
    "\n",
    "# 3) รวมกัน\n",
    "sample100 = pd.concat([head50, random50], ignore_index=True)\n",
    "\n",
    "sample100.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "495d4432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จำนวน unique keys ที่ต้องการ: 177\n"
     ]
    }
   ],
   "source": [
    "keys1 = set(sample100[\"Key_mat_1\"].unique())\n",
    "keys2 = set(sample100[\"Key_mat_2\"].unique())\n",
    "\n",
    "answer_keys = keys1.union(keys2)\n",
    "print(f\"จำนวน unique keys ที่ต้องการ: {len(answer_keys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71ee1dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PO21_PO06131335083', 'PO21_PO06138007044', 'PO21_PO06131335343', 'PO11_PO31130352303', 'PO21_PO06131335105', '7511_75313647080', 'PO21_PO06131335550', 'PO21_PO06138006899'}\n",
      "Missing keys: 8\n"
     ]
    }
   ],
   "source": [
    "mm_keys = set(select_select_mm_matType[\"pk_plant_matnum\"].dropna().unique())\n",
    "missing_keys = answer_keys - mm_keys\n",
    "print(missing_keys)\n",
    "print(f\"Missing keys: {len(missing_keys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "953e738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample100 before: 100\n",
      "sample100 after : 93\n",
      "dropped records : 7\n"
     ]
    }
   ],
   "source": [
    "before = len(sample100)\n",
    "\n",
    "sample100 = sample100[\n",
    "    sample100[\"Key_mat_1\"].isin(mm_keys) &\n",
    "    sample100[\"Key_mat_2\"].isin(mm_keys)\n",
    "].copy()\n",
    "\n",
    "after = len(sample100)\n",
    "\n",
    "print(f\"sample100 before: {before}\")\n",
    "print(f\"sample100 after : {after}\")\n",
    "print(f\"dropped records : {before - after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3580ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จำนวน record ที่มี Key_mat_1 และ Key_mat_2 ครบทั้งคู่: 93\n"
     ]
    }
   ],
   "source": [
    "# สร้าง mask ว่า record ไหนผ่านเงื่อนไข (ต้องมีทั้งคู่) (key ต้องไม่ missing)\n",
    "mask = (\n",
    "    sample100[\"Key_mat_1\"].isin(mm_keys) &\n",
    "    sample100[\"Key_mat_2\"].isin(mm_keys)\n",
    ")\n",
    "\n",
    "# นับจำนวน record ที่ผ่าน\n",
    "count_valid_records = mask.sum()\n",
    "\n",
    "print(\"จำนวน record ที่มี Key_mat_1 และ Key_mat_2 ครบทั้งคู่:\", count_valid_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9943c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "จำนวน unique keys: 163\n",
      "จำนวน record จาก key_mat (answers): 163\n",
      "จำนวน random records: 400\n",
      "จำนวน sample ทั้งหมด: 563\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1) เตรียม key 100 ค่า\n",
    "# ---------------------------\n",
    "keys_200 = pd.concat([\n",
    "    sample100[\"Key_mat_1\"],\n",
    "    sample100[\"Key_mat_2\"]\n",
    "]).dropna().unique()\n",
    "\n",
    "print(f\"จำนวน unique keys: {len(keys_200)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2) ดึง record จาก table หลัก\n",
    "# ---------------------------\n",
    "df_answers = select_select_mm_matType[\n",
    "    select_select_mm_matType[\"pk_plant_matnum\"].isin(keys_200)\n",
    "]\n",
    "\n",
    "print(f\"จำนวน record จาก key_mat (answers): {len(df_answers)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) ตัด record เหล่านี้ออกจาก pool\n",
    "# ---------------------------\n",
    "remaining_pool = select_select_mm_matType[\n",
    "    ~select_select_mm_matType[\"pk_plant_matnum\"].isin(df_answers[\"pk_plant_matnum\"])\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# 4) สุ่มเพิ่ม 200 record (lock seed)\n",
    "# ---------------------------\n",
    "df_400 = remaining_pool.sample(\n",
    "    n=400,\n",
    "    random_state=42   # 🔒 lock seed\n",
    ")\n",
    "\n",
    "print(f\"จำนวน random records: {len(df_400)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5) รวมเป็น sample size 300\n",
    "# ---------------------------\n",
    "\n",
    "sample_500 = pd.concat([df_answers, df_400], ignore_index=True)\n",
    "\n",
    "print(f\"จำนวน sample ทั้งหมด: {len(sample_500)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b5653",
   "metadata": {},
   "source": [
    "## DB (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24aec433",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"black_kalareemhotmail.com\",\n",
    "    \"password\": \"\",\n",
    "    \"options\": \"-c search_path=apm_dev,public -c client_encoding=UTF8\"  # Set default schema and UTF-8 encoding\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3864c575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST: Create Table\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"black_kalareemhotmail.com\",\n",
    "    \"password\": \"\",\n",
    "    \"options\": \"-c search_path=apm_dev,public -c client_encoding=UTF8\"  # Set default schema and UTF-8 encoding\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"spare_part_test\"\n",
    "SCHEMA_NAME = \"apm_dev\"\n",
    "EMBEDDING_DIM = 1024  # bge-m3 dimension\n",
    "\n",
    "# ============================================================\n",
    "# Database Connection\n",
    "# ============================================================\n",
    "def get_connection():\n",
    "    \"\"\"Get database connection with pgvector support\"\"\"\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    conn.set_client_encoding('UTF8')  # Ensure UTF-8 encoding for Thai text\n",
    "    register_vector(conn)\n",
    "    return conn\n",
    "\n",
    "# ============================================================\n",
    "# 1. CREATE TABLE\n",
    "# ============================================================\n",
    "def create_spare_part_table(drop_if_exists: bool = False):\n",
    "    \"\"\"\n",
    "    Create spare_part_test table with all required fields\n",
    "    \n",
    "    Args:\n",
    "        drop_if_exists: If True, drop existing table before creating\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Drop table if requested\n",
    "        if drop_if_exists:\n",
    "            print(f\"⚠️  Dropping existing table {SCHEMA_NAME}.{TABLE_NAME}...\")\n",
    "            cur.execute(f\"DROP TABLE IF EXISTS {SCHEMA_NAME}.{TABLE_NAME} CASCADE\")\n",
    "        \n",
    "        # Create table\n",
    "        create_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.{TABLE_NAME} (\n",
    "            -- Primary Key\n",
    "            pk_plant_matnum TEXT PRIMARY KEY,\n",
    "            \n",
    "            -- Basic Info\n",
    "            material_type TEXT,\n",
    "            plant TEXT,\n",
    "            material_number TEXT,\n",
    "            plant_id TEXT,\n",
    "            plant_description TEXT,\n",
    "            uom TEXT,\n",
    "            \n",
    "            -- Raw Description Fields\n",
    "            material_description TEXT,\n",
    "            po_text TEXT,\n",
    "            \n",
    "            -- ============================================================\n",
    "            -- SEMANTIC SEARCH Fields\n",
    "            -- ============================================================\n",
    "            raw_combine_for_embedding TEXT,\n",
    "            clean_combine_for_embedding TEXT,\n",
    "            embedding vector({EMBEDDING_DIM}),\n",
    "            \n",
    "            -- ============================================================\n",
    "            -- KEYWORD SEARCH Fields (BM25)\n",
    "            -- ============================================================\n",
    "            raw_combine_for_keyword TEXT,\n",
    "            clean_combine_for_keyword TEXT,\n",
    "            tsv tsvector,\n",
    "            \n",
    "            -- Extracted Keywords/Codes\n",
    "            keyword_codes TEXT[],\n",
    "            keyword_old_codes TEXT[],\n",
    "            keyword_list TEXT[],\n",
    "            \n",
    "            -- Metadata\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"📦 Creating table {SCHEMA_NAME}.{TABLE_NAME}...\")\n",
    "        cur.execute(create_sql)\n",
    "        \n",
    "        # Enable required extensions\n",
    "        print(\"🔧 Ensuring extensions are enabled...\")\n",
    "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS pg_trgm\")\n",
    "        \n",
    "        # Create indexes\n",
    "        print(\"🔍 Creating indexes...\")\n",
    "        \n",
    "        # A) Vector index for semantic search\n",
    "        cur.execute(f\"\"\"\n",
    "            DO $$\n",
    "            BEGIN\n",
    "                IF NOT EXISTS (\n",
    "                    SELECT 1 FROM pg_indexes \n",
    "                    WHERE schemaname='{SCHEMA_NAME}' \n",
    "                    AND tablename='{TABLE_NAME}' \n",
    "                    AND indexname='idx_{TABLE_NAME}_embedding'\n",
    "                ) THEN\n",
    "                    CREATE INDEX idx_{TABLE_NAME}_embedding \n",
    "                    ON {SCHEMA_NAME}.{TABLE_NAME} \n",
    "                    USING ivfflat (embedding vector_cosine_ops)\n",
    "                    WITH (lists = 100);\n",
    "                END IF;\n",
    "            END\n",
    "            $$;\n",
    "        \"\"\")\n",
    "        \n",
    "        # B) Full-text search index\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_tsv \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} \n",
    "            USING gin(tsv);\n",
    "        \"\"\")\n",
    "        \n",
    "        # C) Trigram index\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_pk_trgm \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} \n",
    "            USING gin (pk_plant_matnum gin_trgm_ops);\n",
    "        \"\"\")\n",
    "        \n",
    "        # D) Array indexes for keyword matching\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_keyword_codes \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} \n",
    "            USING gin (keyword_codes);\n",
    "            \n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_keyword_list \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} \n",
    "            USING gin (keyword_list);\n",
    "        \"\"\")\n",
    "        \n",
    "        # E) Basic indexes\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_material_type \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} (material_type);\n",
    "            \n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_uom \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} (uom);\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create trigger for auto-updating tsv\n",
    "        print(\"⚙️  Creating trigger for tsvector auto-update...\")\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE OR REPLACE FUNCTION {SCHEMA_NAME}.{TABLE_NAME}_tsv_trigger() \n",
    "            RETURNS TRIGGER AS $$\n",
    "            BEGIN\n",
    "                NEW.tsv := to_tsvector('simple', COALESCE(NEW.clean_combine_for_keyword, ''));\n",
    "                NEW.updated_at := CURRENT_TIMESTAMP;\n",
    "                RETURN NEW;\n",
    "            END;\n",
    "            $$ LANGUAGE plpgsql;\n",
    "            \n",
    "            DROP TRIGGER IF EXISTS tsv_update_trigger ON {SCHEMA_NAME}.{TABLE_NAME};\n",
    "            \n",
    "            CREATE TRIGGER tsv_update_trigger \n",
    "            BEFORE INSERT OR UPDATE ON {SCHEMA_NAME}.{TABLE_NAME}\n",
    "            FOR EACH ROW \n",
    "            EXECUTE FUNCTION {SCHEMA_NAME}.{TABLE_NAME}_tsv_trigger();\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"✅ Table {SCHEMA_NAME}.{TABLE_NAME} created successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"❌ Error creating table: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "# Test\n",
    "print(\"=\"*60)\n",
    "print(\"TEST: Create Table\")\n",
    "print(\"=\"*60)\n",
    "# get_connection()\n",
    "# create_spare_part_table(drop_if_exists=True)  # Uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c1ab396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST: BGE-M3 Embedder\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. EMBEDDER CLASS (BGE-M3)\n",
    "# ============================================================\n",
    "class BGE_M3_Embedder:\n",
    "    \"\"\"\n",
    "    BGE-M3 Embedding Model Wrapper\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-m3\", device: str = None):\n",
    "        \"\"\"\n",
    "        Initialize BGE-M3 model\n",
    "        \n",
    "        Args:\n",
    "            model_name: Model identifier (default: BAAI/bge-m3)\n",
    "            device: 'cuda', 'cpu', or None (auto-detect)\n",
    "        \"\"\"\n",
    "        print(f\"🔄 Loading embedding model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.dim = 1024  # bge-m3 output dimension\n",
    "        print(f\"✅ Model loaded! Dimension: {self.dim}\")\n",
    "    \n",
    "    def embed_texts(\n",
    "        self, \n",
    "        texts: List[str], \n",
    "        batch_size: int = 32,\n",
    "        show_progress: bool = True\n",
    "    ) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            batch_size: Batch size for encoding\n",
    "            show_progress: Show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            List of embedding vectors (each is list of floats)\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=show_progress,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True  # Recommended for bge-m3\n",
    "        )\n",
    "        \n",
    "        return [emb.tolist() for emb in embeddings]\n",
    "    \n",
    "    def embed_single(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single text (convenience method)\"\"\"\n",
    "        return self.embed_texts([text], show_progress=False)[0]\n",
    "\n",
    "# Test\n",
    "print(\"=\"*60)\n",
    "print(\"TEST: BGE-M3 Embedder\")\n",
    "print(\"=\"*60)\n",
    "# embedder = BGE_M3_Embedder()  # Uncomment to test\n",
    "# test_emb = embedder.embed_single(\"BEARING BALL TEST\")\n",
    "# print(f\"Embedding dim: {len(test_emb)}\")\n",
    "# print(f\"First 5 values: {test_emb[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89019533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST: Create Table & Insert Data\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. INSERT DATA WITH EMBEDDINGS\n",
    "# ============================================================\n",
    "def insert_spare_parts(\n",
    "    df: pd.DataFrame, \n",
    "    embedder: BGE_M3_Embedder, \n",
    "    batch_size: int = 128,\n",
    "    start_from: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Insert spare part data into database with embeddings\n",
    "    \n",
    "    Required DataFrame columns:\n",
    "        - pk_plant_matnum (PRIMARY KEY)\n",
    "        - material_type, plant, material_number, plant_id, plant_description, uom\n",
    "        - material_description, po_text\n",
    "        - raw_combine_for_embedding, clean_combine_for_embedding\n",
    "        - raw_combine_for_keyword, clean_combine_for_keyword\n",
    "        - keyword_codes (list), keyword_old_codes (list), keyword_list (list)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with spare part data\n",
    "        embedder: BGE_M3_Embedder instance\n",
    "        batch_size: Number of records to process per batch\n",
    "        start_from: Skip first N records (for resuming)\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Normalize column names to lowercase\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Required columns check\n",
    "    required_cols = [\n",
    "        'pk_plant_matnum', 'material_type', 'plant', 'material_number', \n",
    "        'plantid', 'plant_description', 'uom', 'material_description', 'text',\n",
    "        'raw_combine_for_embedding', 'clean_combine_for_embedding',\n",
    "        'raw_combine_for_keyword', 'clean_combine_for_keyword',\n",
    "        'keyword_codes', 'keyword_old_codes', 'keyword_list'\n",
    "    ]\n",
    "    \n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    df_to_insert = df[required_cols].copy()\n",
    "    total_rows = len(df_to_insert)\n",
    "    \n",
    "    if start_from > 0:\n",
    "        df_to_insert = df_to_insert.iloc[start_from:]\n",
    "        print(f\"⏭️  Skipping first {start_from} records...\")\n",
    "    \n",
    "    print(f\"📊 Total records to insert: {len(df_to_insert)}\")\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "        INSERT INTO {SCHEMA_NAME}.{TABLE_NAME} (\n",
    "            pk_plant_matnum, material_type, plant, material_number,\n",
    "            plant_id, plant_description, uom, material_description, po_text,\n",
    "            raw_combine_for_embedding, clean_combine_for_embedding, embedding,\n",
    "            raw_combine_for_keyword, clean_combine_for_keyword,\n",
    "            keyword_codes, keyword_old_codes, keyword_list\n",
    "        ) VALUES (\n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n",
    "        )\n",
    "        ON CONFLICT (pk_plant_matnum) \n",
    "        DO UPDATE SET\n",
    "            material_type = EXCLUDED.material_type,\n",
    "            material_description = EXCLUDED.material_description,\n",
    "            clean_combine_for_embedding = EXCLUDED.clean_combine_for_embedding,\n",
    "            clean_combine_for_keyword = EXCLUDED.clean_combine_for_keyword,\n",
    "            embedding = EXCLUDED.embedding,\n",
    "            keyword_codes = EXCLUDED.keyword_codes,\n",
    "            keyword_list = EXCLUDED.keyword_list,\n",
    "            updated_at = CURRENT_TIMESTAMP\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        inserted = 0\n",
    "        failed = 0\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(df_to_insert), batch_size), desc=\"Inserting batches\"):\n",
    "            batch_df = df_to_insert.iloc[i:i+batch_size]\n",
    "            \n",
    "            # Generate embeddings for this batch\n",
    "            texts_to_embed = batch_df['clean_combine_for_embedding'].fillna(\"\").tolist()\n",
    "            embeddings = embedder.embed_texts(texts_to_embed, show_progress=False)\n",
    "            \n",
    "            # Prepare batch data\n",
    "            batch_data = []\n",
    "            for idx, (_, row) in enumerate(batch_df.iterrows()):\n",
    "                # Handle column name variations\n",
    "                plant_id = row.get('plant_id') or row.get('plantid', '')\n",
    "                po_text = row.get('po_text') or row.get('text', '')\n",
    "                \n",
    "                batch_data.append((\n",
    "                    row['pk_plant_matnum'],\n",
    "                    row['material_type'],\n",
    "                    row['plant'],\n",
    "                    row['material_number'],\n",
    "                    plant_id,\n",
    "                    row['plant_description'],\n",
    "                    row['uom'],\n",
    "                    row['material_description'],\n",
    "                    po_text,\n",
    "                    row['raw_combine_for_embedding'],\n",
    "                    row['clean_combine_for_embedding'],\n",
    "                    embeddings[idx],  # embedding vector\n",
    "                    row['raw_combine_for_keyword'],\n",
    "                    row['clean_combine_for_keyword'],\n",
    "                    row['keyword_codes'] if isinstance(row['keyword_codes'], list) else [],\n",
    "                    row['keyword_old_codes'] if isinstance(row['keyword_old_codes'], list) else [],\n",
    "                    row['keyword_list'] if isinstance(row['keyword_list'], list) else []\n",
    "                ))\n",
    "            \n",
    "            # Execute batch insert\n",
    "            try:\n",
    "                psycopg2.extras.execute_batch(cur, insert_sql, batch_data, page_size=batch_size)\n",
    "                conn.commit()\n",
    "                inserted += len(batch_data)\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                failed += len(batch_data)\n",
    "                print(f\"\\n❌ Batch {i//batch_size + 1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n✅ Insertion complete!\")\n",
    "        print(f\"   Inserted: {inserted} records\")\n",
    "        if failed > 0:\n",
    "            print(f\"   Failed: {failed} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"❌ Error during insertion: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "# Test\n",
    "print(\"=\"*60)\n",
    "print(\"TEST: Create Table & Insert Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "\n",
    "# # # Time: Create table\n",
    "# print(\"\\n🏗️  Creating table...\")\n",
    "# start_time_table = time.time()\n",
    "# create_spare_part_table(drop_if_exists=True)\n",
    "# time_table = time.time() - start_time_table\n",
    "# print(f\"✅ Table created in {time_table:.2f} seconds\\n\")\n",
    "\n",
    "# # Time: Insert data with embeddings\n",
    "# print(\"📥 Inserting data with embeddings...\")\n",
    "# start_time_insert = time.time()\n",
    "# embedder = BGE_M3_Embedder()\n",
    "# insert_spare_parts(sample_500, embedder, batch_size=32)\n",
    "# time_insert = time.time() - start_time_insert\n",
    "# print(f\"✅ Data inserted in {time_insert:.2f} seconds\\n\")\n",
    "\n",
    "# print(f\"⏱️  Total time: {time_table + time_insert:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9118d8c",
   "metadata": {},
   "source": [
    "## Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83e8aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4A. VECTOR SEARCH QUERY (SEMANTIC ONLY)\n",
    "# ============================================================\n",
    "\n",
    "def vector_search(\n",
    "    query_text: str,\n",
    "    embedder: BGE_M3_Embedder,\n",
    "    top_k_semantic: int = 100,\n",
    "    limit: int = 50,\n",
    "    uom_filter: Optional[str] = None,\n",
    "    material_type_filter: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform pure vector (semantic) search using embeddings only\n",
    "\n",
    "    Args:\n",
    "        query_text: Search query text (for embedding)\n",
    "        embedder: BGE_M3_Embedder instance\n",
    "        top_k_semantic: Top K results from vector search\n",
    "        limit: Final result limit\n",
    "        uom_filter: Optional UOM filter (e.g., 'PC')\n",
    "        material_type_filter: Optional material type filter\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with search results and semantic scores\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) Generate query embedding\n",
    "    # ------------------------------------------------------------\n",
    "    query_embedding = embedder.embed_single(query_text)\n",
    "\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "\n",
    "    try:\n",
    "        # ------------------------------------------------------------\n",
    "        # 2) Build optional filters\n",
    "        # ------------------------------------------------------------\n",
    "        filter_clause = \"\"\n",
    "        if uom_filter:\n",
    "            filter_clause += f\" AND uom = '{uom_filter}'\"\n",
    "        if material_type_filter:\n",
    "            filter_clause += f\" AND material_type = '{material_type_filter}'\"\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 3) Vector search SQL\n",
    "        # ------------------------------------------------------------\n",
    "        search_sql = f\"\"\"\n",
    "        WITH semantic_search AS (\n",
    "            SELECT\n",
    "                pk_plant_matnum,\n",
    "                material_description,\n",
    "                uom,\n",
    "                (1 - (embedding <=> %s::vector)) AS semantic_score\n",
    "            FROM {SCHEMA_NAME}.{TABLE_NAME}\n",
    "            WHERE embedding IS NOT NULL\n",
    "                {filter_clause}\n",
    "            ORDER BY embedding <=> %s::vector\n",
    "            LIMIT {top_k_semantic}\n",
    "        ),\n",
    "\n",
    "        normalized_scores AS (\n",
    "            SELECT\n",
    "                pk_plant_matnum,\n",
    "                material_description,\n",
    "                uom,\n",
    "                semantic_score,\n",
    "                CASE\n",
    "                    WHEN semantic_score > 0\n",
    "                        THEN semantic_score / MAX(semantic_score) OVER ()\n",
    "                    ELSE 0\n",
    "                END AS semantic_norm\n",
    "            FROM semantic_search\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "            pk_plant_matnum,\n",
    "            material_description,\n",
    "            uom,\n",
    "            semantic_norm AS final_score,\n",
    "            ROUND(semantic_norm::numeric, 3) AS semantic_score\n",
    "        FROM normalized_scores\n",
    "        ORDER BY final_score DESC\n",
    "        LIMIT {limit};\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 4) Execute query\n",
    "        # ------------------------------------------------------------\n",
    "        cur.execute(search_sql, (\n",
    "            query_embedding,\n",
    "            query_embedding\n",
    "        ))\n",
    "\n",
    "        results = cur.fetchall()\n",
    "        df_results = pd.DataFrame(results)\n",
    "        return df_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Vector search error: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185655be",
   "metadata": {},
   "source": [
    "### Evaluate vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "503a35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATION: VECTOR SEARCH QUALITY\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_vector_search_quality(\n",
    "    edges_df,\n",
    "    sample_df,\n",
    "    embedder,\n",
    "    top_k=30\n",
    "):\n",
    "    \"\"\"\n",
    "    ประเมินคุณภาพของ vector (semantic) search\n",
    "    โดยเช็คว่าเฉลยอยู่ที่ rank เท่าไหร่\n",
    "\n",
    "    Args:\n",
    "        edges_df: DataFrame ที่มี Key_mat_1, Key_mat_2 (คู่ที่เป็นเฉลย)\n",
    "        sample_df: DataFrame ที่มีข้อมูล material ทั้งหมด (sample_500)\n",
    "        embedder: BGE_M3_Embedder instance\n",
    "        top_k: จำนวน candidates สูงสุดที่จะค้นหา\n",
    "\n",
    "    Returns:\n",
    "        DataFrame สรุปผลการประเมิน\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) Build answer map (pk -> list of correct answers)\n",
    "    # ------------------------------------------------------------\n",
    "    answer_map = {}\n",
    "    for _, row in edges_df.iterrows():\n",
    "        key1 = row['Key_mat_1']\n",
    "        key2 = row['Key_mat_2']\n",
    "\n",
    "        if pd.notna(key1) and pd.notna(key2):\n",
    "            answer_map.setdefault(key1, []).append(key2)\n",
    "            answer_map.setdefault(key2, []).append(key1)\n",
    "\n",
    "    print(f\"📊 Total PKs with answers: {len(answer_map)}\")\n",
    "    print(f\"🔍 Starting VECTOR search evaluation\")\n",
    "    print(f\"⏱️  This may take a few minutes...\\n\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2) Helper: case-insensitive column getter\n",
    "    # ------------------------------------------------------------\n",
    "    def get_col(rec, col_name):\n",
    "        col_lower = col_name.lower()\n",
    "        for col in rec.index:\n",
    "            if col.lower() == col_lower:\n",
    "                return rec[col]\n",
    "        return ''\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3) Evaluate each PK\n",
    "    # ------------------------------------------------------------\n",
    "    for pk, answers in tqdm(answer_map.items(), desc=\"Evaluating PKs (Vector Search)\"):\n",
    "        record_rows = sample_df[sample_df['pk_plant_matnum'] == pk]\n",
    "\n",
    "        if len(record_rows) == 0:\n",
    "            continue\n",
    "\n",
    "        record = record_rows.iloc[0]\n",
    "\n",
    "        query_text = get_col(record, 'clean_combine_for_embedding')\n",
    "        material_desc = get_col(record, 'material_description')\n",
    "\n",
    "        if not query_text or pd.isna(query_text):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # ----------------------------------------------------\n",
    "            # Vector search\n",
    "            # ----------------------------------------------------\n",
    "            candidates = vector_search(\n",
    "                query_text=query_text,\n",
    "                embedder=embedder,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            candidate_pks = candidates['pk_plant_matnum'].tolist()\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # Find ranks of correct answers\n",
    "            # ----------------------------------------------------\n",
    "            found_ranks = []\n",
    "            for answer_pk in answers:\n",
    "                if answer_pk in candidate_pks:\n",
    "                    rank = candidate_pks.index(answer_pk) + 1\n",
    "                    found_ranks.append(rank)\n",
    "\n",
    "            if found_ranks:\n",
    "                best_rank = min(found_ranks)\n",
    "                status = \"✅ Found\"\n",
    "            else:\n",
    "                best_rank = None\n",
    "                status = \"❌ Not Found\"\n",
    "\n",
    "            results.append({\n",
    "                'pk': pk,\n",
    "                'material_description': material_desc[:50] if material_desc else '',\n",
    "                'num_answers': len(answers),\n",
    "                'answers': ', '.join(answers),\n",
    "                'found_ranks': ', '.join(map(str, found_ranks)) if found_ranks else 'N/A',\n",
    "                'best_rank': best_rank,\n",
    "                'status': status\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  Error processing {pk}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4) Results DataFrame\n",
    "    # ------------------------------------------------------------\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b98e010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION: Vector Search Quality Assessment\n",
      "================================================================================\n",
      "🔄 Loading embedding model: BAAI/bge-m3...\n",
      "✅ Model loaded! Dimension: 1024\n",
      "📊 Total PKs with answers: 163\n",
      "🔍 Starting VECTOR search evaluation\n",
      "⏱️  This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PKs (Vector Search): 100%|██████████| 163/163 [00:10<00:00, 15.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation completed in 17.38 seconds\n",
      "                    pk                      material_description  num_answers  \\\n",
      "0  7560_75060602310346  BEARING,ANGULAR CONTACT,3310 ATN9  [VMI]            1   \n",
      "1  7511_75060610300414      BEARING,ANGULAR CONTACT,3310 ATN9(F)            1   \n",
      "2  7560_75060602411346    BEARING,ANGULAR CONTACT,3311 ANR   [F]            1   \n",
      "3  7511_75060610310502       BEARING,ANGULAR CONTACT,3311 ANR(F)            1   \n",
      "4  7812_78060606306581  BEARING,ANGULAR CONTACT,7306 BECBM [VMI]            1   \n",
      "\n",
      "               answers found_ranks  best_rank   status  \n",
      "0  7511_75060610300414           3        3.0  ✅ Found  \n",
      "1  7560_75060602310346           4        4.0  ✅ Found  \n",
      "2  7511_75060610310502           4        4.0  ✅ Found  \n",
      "3  7560_75060602411346           3        3.0  ✅ Found  \n",
      "4  7511_75060610805312           2        2.0  ✅ Found  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EVALUATION: Vector Search Quality Assessment\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "start_time_eval = time.time()\n",
    "\n",
    "embedder = BGE_M3_Embedder()\n",
    "\n",
    "evaluation_results_vec = evaluate_vector_search_quality(\n",
    "    edges_df=sample100,\n",
    "    sample_df=sample_500,\n",
    "    embedder=embedder,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "time_eval = time.time() - start_time_eval\n",
    "\n",
    "print(f\"\\n✅ Evaluation completed in {time_eval:.2f} seconds\")\n",
    "print(evaluation_results_vec.head())\n",
    "# print(f\"⚡ Average time per query: {time_eval/len(evaluation_results_vec):.3f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06daf3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📈 SUMMARY STATISTICS (VECTOR SEARCH)\n",
      "================================================================================\n",
      "Total PKs evaluated: 163\n",
      "Found in candidates: 159 (97.5%)\n",
      "Not found: 4 (2.5%)\n",
      "\n",
      "Rank Statistics (for found answers):\n",
      "  Mean Rank: 7.48\n",
      "  Median Rank: 3.0\n",
      "  Best Rank: 2.0\n",
      "  Worst Rank: 45.0\n",
      "\n",
      "📊 Rank Distribution:\n",
      "  Top 1: 0\n",
      "  Rank 2–3: 81\n",
      "  Rank 4–5: 19\n",
      "  Rank 6–10: 25\n",
      "  Rank 11–20: 22\n",
      "  Rank 21–25: 2\n",
      "  Rank 26–30: 2\n",
      "  Rank 31–40: 3\n",
      "  Over 40: 5\n",
      "\n",
      "✅ Total counted: 159\n",
      "📦 Total rows: 159\n",
      "\n",
      "================================================================================\n",
      "❌ CASES WHERE ANSWER NOT FOUND (VECTOR SEARCH)\n",
      "================================================================================\n",
      "                 pk                    material_description            answers\n",
      "7511_75060621856000       BEARING,DEEP GROOVE,608-2Z,SKF(F) PO11_PO06131330604\n",
      "7560_75060625503999       BEARING,DEEP GROOVE,6303,SKF  [5] PO21_PO06131330912\n",
      "7560_75060624704999 BEARING,DEEP GROOVE,6204-2Z/C3,SKF  [F] PO21_PO06131330930\n",
      "7813_78060617605000   BEARING,DEEP GROOVE,6303-2Z/C3,SKF(F) PO21_PO06131330940\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SUMMARY STATISTICS: VECTOR SEARCH\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📈 SUMMARY STATISTICS (VECTOR SEARCH)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total = len(evaluation_results_vec)\n",
    "found = len(evaluation_results_vec[evaluation_results_vec['status'] == '✅ Found'])\n",
    "not_found = total - found\n",
    "\n",
    "print(f\"Total PKs evaluated: {total}\")\n",
    "print(f\"Found in candidates: {found} ({found/total*100:.1f}%)\")\n",
    "print(f\"Not found: {not_found} ({not_found/total*100:.1f}%)\")\n",
    "\n",
    "if found > 0:\n",
    "    found_df = evaluation_results_vec[\n",
    "        evaluation_results_vec['status'] == '✅ Found'\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nRank Statistics (for found answers):\")\n",
    "    print(f\"  Mean Rank: {found_df['best_rank'].mean():.2f}\")\n",
    "    print(f\"  Median Rank: {found_df['best_rank'].median():.1f}\")\n",
    "    print(f\"  Best Rank: {found_df['best_rank'].min()}\")\n",
    "    print(f\"  Worst Rank: {found_df['best_rank'].max()}\")\n",
    "\n",
    "    total_found = len(found_df)\n",
    "\n",
    "    top_1 = len(found_df[found_df['best_rank'] == 1])\n",
    "    top_2_3 = len(found_df[(found_df['best_rank'] >= 2) & (found_df['best_rank'] <= 3)])\n",
    "    top_4_5 = len(found_df[(found_df['best_rank'] >= 4) & (found_df['best_rank'] <= 5)])\n",
    "    top_6_10 = len(found_df[(found_df['best_rank'] >= 6) & (found_df['best_rank'] <= 10)])\n",
    "    top_11_20 = len(found_df[(found_df['best_rank'] >= 11) & (found_df['best_rank'] <= 20)])\n",
    "    top_21_25 = len(found_df[(found_df['best_rank'] >= 21) & (found_df['best_rank'] <= 25)])\n",
    "    top_26_30 = len(found_df[(found_df['best_rank'] >= 26) & (found_df['best_rank'] <= 30)])\n",
    "    top_31_40 = len(found_df[(found_df['best_rank'] >= 31) & (found_df['best_rank'] <= 40)])\n",
    "    over_40 = len(found_df[found_df['best_rank'] > 40])\n",
    "\n",
    "    print(f\"\\n📊 Rank Distribution:\")\n",
    "    print(f\"  Top 1: {top_1}\")\n",
    "    print(f\"  Rank 2–3: {top_2_3}\")\n",
    "    print(f\"  Rank 4–5: {top_4_5}\")\n",
    "    print(f\"  Rank 6–10: {top_6_10}\")\n",
    "    print(f\"  Rank 11–20: {top_11_20}\")\n",
    "    print(f\"  Rank 21–25: {top_21_25}\")\n",
    "    print(f\"  Rank 26–30: {top_26_30}\")\n",
    "    print(f\"  Rank 31–40: {top_31_40}\")\n",
    "    print(f\"  Over 40: {over_40}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\n✅ Total counted: \"\n",
    "        f\"{top_1 + top_2_3 + top_4_5 + top_6_10 + top_11_20 + top_21_25 + top_26_30 + top_31_40 + over_40}\"\n",
    "    )\n",
    "    print(f\"📦 Total rows: {total_found}\")\n",
    "\n",
    "# ============================================================\n",
    "# CASES WHERE ANSWER NOT FOUND\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"❌ CASES WHERE ANSWER NOT FOUND (VECTOR SEARCH)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "not_found_df = evaluation_results_vec[\n",
    "    evaluation_results_vec['status'] == '❌ Not Found'\n",
    "]\n",
    "\n",
    "if len(not_found_df) > 0:\n",
    "    print(\n",
    "        not_found_df[['pk', 'material_description', 'answers']]\n",
    "        .to_string(index=False)\n",
    "    )\n",
    "else:\n",
    "    print(\"🎉 All answers were found in candidates!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6c2b7",
   "metadata": {},
   "source": [
    "## Prepare data for LLM-Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0bbe61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6e4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fc373ff",
   "metadata": {},
   "source": [
    "## LLM-Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b08c4",
   "metadata": {},
   "source": [
    "### ✅ LLM Material Grouping - UPDATED VERSION\n",
    "\n",
    "**สิ่งที่ปรับปรุง:**\n",
    "\n",
    "1. **Output Schema ที่เรียบง่าย** - เอาเฉพาะข้อมูลที่จำเป็น:\n",
    "   - `status`: \"match\" หรือ \"unmatch\" \n",
    "   - `matched_pks`: list ของ PKs ที่เป็น duplicate (ไม่รวม query_pk เอง)\n",
    "   - `summary`: สรุปสั้นๆ\n",
    "   \n",
    "2. **Prompt ที่ชัดเจนขึ้น** - บอก LLM ให้ return เฉพาะ PKs ที่ซ้ำกัน ไม่ต้องส่ง analysis แต่ละตัว\n",
    "\n",
    "3. **Validation ที่ถูกต้อง** - ใช้ PKs จริงจาก edges.csv:\n",
    "   - 7560_75060675913234 (ไม่ใช่ 7560_75060670873346)\n",
    "   - PO11_PO06131335013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d49854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPROVED LLM Function with Step-by-Step Reasoning\n",
    "# ============================================================\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class MatchedMaterial(BaseModel):\n",
    "    \"\"\"Material ที่ตรงกับ query\"\"\"\n",
    "    pk: str = Field(description=\"PK ของ candidate ที่เป็น duplicate\")\n",
    "    confidence: float = Field(description=\"ความมั่นใจ 0.0-1.0\", ge=0.0, le=1.0)\n",
    "    reason: str = Field(description=\"เหตุผลที่ตรงกัน\")\n",
    "\n",
    "class ImprovedMaterialGroupingResult(BaseModel):\n",
    "    \"\"\"ผลลัพธ์การจัดกลุ่ม materials แบบปรับปรุง\"\"\"\n",
    "    query_pk: str\n",
    "    status: str  # \"match\" หรือ \"unmatch\"\n",
    "    matched_materials: List[MatchedMaterial] = Field(default_factory=list)\n",
    "    step_by_step_reasoning: str  # Reasoning แบบทีละขั้นตอน\n",
    "    summary: str\n",
    "\n",
    "def improved_group_materials_with_llm(\n",
    "    query_pk: str,\n",
    "    query_description: str,\n",
    "    query_full_text: str,\n",
    "    candidates: pd.DataFrame,\n",
    "    few_shot_examples: List[dict],\n",
    "    model: str = \"gemini-3-flash-preview\",\n",
    "    min_confidence: float = 0.7\n",
    ") -> ImprovedMaterialGroupingResult:\n",
    "    \"\"\"\n",
    "    ปรับปรุง LLM grouping ด้วย step-by-step reasoning และ few-shot examples\n",
    "    \"\"\"\n",
    "    \n",
    "    # เตรียม client\n",
    "    client = genai.Client(\n",
    "        vertexai={\n",
    "            \"project\": \"prj-service-mlops\",\n",
    "            \"location\": \"global\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # เตรียม candidates\n",
    "    candidate_list = []\n",
    "    for idx, row in candidates.iterrows():\n",
    "        candidate_list.append({\n",
    "            \"pk\": row['pk_plant_matnum'],\n",
    "            \"description\": row['material_description'],\n",
    "            \"full_text\": row.get('clean_combine_for_keyword', row['material_description'])[:500]\n",
    "        })\n",
    "    \n",
    "    # สร้าง Few-Shot Examples section\n",
    "    examples_text = \"\"\n",
    "    for i, ex in enumerate(few_shot_examples, 1):\n",
    "        examples_text += f\"\"\"\n",
    "### Example {i}: Materials that ARE Duplicates\n",
    "\n",
    "**Query Material:**\n",
    "- PK: {ex['query_pk']}\n",
    "- Description: {ex['query_description']}\n",
    "- Full Text: {ex['clean_combine_for_keyword']}\n",
    "\n",
    "**Candidate Material (DUPLICATE):**\n",
    "- PK: {ex['answer_pk']}\n",
    "- Description: {ex['answer_description']}\n",
    "- Full Text: {ex['clean_combine_for_keyword']}\n",
    "\n",
    "**Why they are duplicates:** Despite different wording, they refer to the same physical product. Key identifiers and specifications match.\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "    \n",
    "    # สร้าง prompt ใหม่\n",
    "    prompt = f\"\"\"# Advanced Material Deduplication with Step-by-Step Analysis\n",
    "\n",
    "You are an expert in industrial spare parts identification. Your task is to identify which candidate materials are EXACT DUPLICATES of the query material using systematic analysis.\n",
    "\n",
    "## 📋 Few-Shot Examples (Learn from these):\n",
    "{examples_text}\n",
    "\n",
    "## 🎯 Current Task:\n",
    "\n",
    "### Query Material (Find duplicates of this):\n",
    "- **PK**: {query_pk}\n",
    "- **Description**: {query_description}\n",
    "- **Full Text**: {query_full_text[:800]}\n",
    "\n",
    "### Candidate Materials (Analyze each one):\n",
    "\"\"\"\n",
    "    \n",
    "    for i, cand in enumerate(candidate_list, 1):\n",
    "        prompt += f\"\"\"\n",
    "**Candidate #{i}:**\n",
    "- PK: {cand['pk']}\n",
    "- Description: {cand['description']}\n",
    "- Full Text: {cand['full_text']}\n",
    "\"\"\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "\n",
    "## 🔍 Analysis Framework (Follow these steps):\n",
    "\n",
    "**Step 1: Extract Key Identifiers**\n",
    "- Part numbers, model codes, manufacturer codes\n",
    "- Size/dimensions (numbers with units like mm, cm, inch)\n",
    "- Material type (steel, plastic, rubber, etc.)\n",
    "- Capacity/rating (voltage, pressure, flow rate)\n",
    "\n",
    "**Step 2: Compare Technical Specifications**\n",
    "- Are dimensions identical or equivalent?\n",
    "- Same material composition?\n",
    "- Same technical ratings?\n",
    "- Same functionality?\n",
    "\n",
    "**Step 3: Evaluate Semantic Similarity**\n",
    "- Do descriptions refer to the same product?\n",
    "- Are they used for the same purpose?\n",
    "- Same application area?\n",
    "\n",
    "**Step 4: Determine Confidence Level**\n",
    "- **High (0.9-1.0)**: Identical part numbers AND specs\n",
    "- **Medium-High (0.8-0.89)**: Same specs, similar codes\n",
    "- **Medium (0.7-0.79)**: Same specs, different codes\n",
    "- **Below 0.7**: Uncertain or different materials\n",
    "\n",
    "**Step 5: Make Final Decision**\n",
    "- Only include matches with confidence ≥ 0.7\n",
    "- When uncertain, be conservative (don't match)\n",
    "\n",
    "## ✅ Matching Rules:\n",
    "\n",
    "**MATCH if:**\n",
    "- Same part number/model code (even with plant prefix)\n",
    "- Same specifications (size, material, capacity)\n",
    "- Different only in: plant code, vendor ID, packaging info\n",
    "\n",
    "**DO NOT MATCH if:**\n",
    "- Different part numbers AND different specs\n",
    "- Different sizes or capacities\n",
    "- Different materials or types\n",
    "- One is accessory, another is main product\n",
    "- Similar but not identical\n",
    "\n",
    "## 📊 Output Format:\n",
    "\n",
    "For each duplicate found, provide:\n",
    "1. **pk**: The candidate PK\n",
    "2. **confidence**: 0.7-1.0 (≥0.7 to include)\n",
    "3. **reason**: Brief explanation (30-50 words)\n",
    "\n",
    "Include **step_by_step_reasoning** showing your analysis process.\n",
    "\n",
    "## ⚠️ Important Notes:\n",
    "\n",
    "- Be systematic - analyze ALL candidates\n",
    "- Be conservative - only high confidence matches\n",
    "- Ignore the first candidate if it's the query itself (same PK)\n",
    "- Focus on TECHNICAL match, not just keyword similarity\n",
    "- Different plants can have same material\n",
    "\n",
    "Analyze carefully and provide your results.\"\"\"\n",
    "\n",
    "    # เรียก LLM\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=ImprovedMaterialGroupingResult,\n",
    "            temperature=0.1,  # เพิ่มเล็กน้อยเพื่อความยืดหยุ่น\n",
    "            top_p=0.95\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    result = ImprovedMaterialGroupingResult.model_validate_json(response.text)\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    filtered_matches = [\n",
    "        m for m in result.matched_materials \n",
    "        if m.confidence >= min_confidence\n",
    "    ]\n",
    "    \n",
    "    result.matched_materials = filtered_matches\n",
    "    if not filtered_matches:\n",
    "        result.status = \"unmatch\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✅ Improved LLM function created\")\n",
    "print(f\"   Model: gemini-3-flash-preview\")\n",
    "print(f\"   Min confidence: 0.7\")\n",
    "print(f\"   Features: Step-by-step reasoning + Few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6facf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ทดสอบ Improved LLM กับ Test Cases\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🧪 TESTING IMPROVED LLM (gemini-3-flash-preview)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "improved_llm_results = []\n",
    "\n",
    "for i, test_result in enumerate(test_results, 1):\n",
    "    print(f\"\\n[{i}/{len(test_results)}] Case {test_result['case_number']}: {test_result['query_pk'][:40]}...\")\n",
    "    \n",
    "    # ดึง query info\n",
    "    query_pk = test_result['query_pk']\n",
    "    query_description = test_result['query_description']\n",
    "    \n",
    "    query_record = sample_500[sample_500['pk_plant_matnum'] == query_pk].iloc[0]\n",
    "    query_full_text = query_record.get('clean_combine_for_keyword', query_description)\n",
    "    \n",
    "    # แปลง candidates เป็น DataFrame\n",
    "    candidates_df = pd.DataFrame(test_result['candidates'])\n",
    "    \n",
    "    try:\n",
    "        # ส่งให้ Improved LLM\n",
    "        print(f\"   📤 Sending to improved LLM...\")\n",
    "        \n",
    "        improved_result = improved_group_materials_with_llm(\n",
    "            query_pk=query_pk,\n",
    "            query_description=query_description,\n",
    "            query_full_text=query_full_text,\n",
    "            candidates=candidates_df,\n",
    "            few_shot_examples=selected_examples,\n",
    "            model=\"gemini-3-flash-preview\",\n",
    "            min_confidence=0.7\n",
    "        )\n",
    "        \n",
    "        # เก็บผลลัพธ์\n",
    "        result_dict = {\n",
    "            \"case_number\": test_result['case_number'],\n",
    "            \"query_pk\": test_result['query_pk'],\n",
    "            \"has_answer\": test_result['has_answer'],\n",
    "            \"answer_pk\": test_result['answer_pk'],\n",
    "            \"answer_in_candidates\": test_result.get('answer_in_candidates'),\n",
    "            \"answer_rank\": test_result.get('answer_rank'),\n",
    "            \"llm_response\": improved_result,\n",
    "            \"matched_pks\": [m.pk for m in improved_result.matched_materials]\n",
    "        }\n",
    "        \n",
    "        # แสดงผลลัพธ์\n",
    "        if improved_result.status == \"match\" and improved_result.matched_materials:\n",
    "            print(f\"   ✅ Status: {improved_result.status}\")\n",
    "            print(f\"   📋 Found {len(improved_result.matched_materials)} matches:\")\n",
    "            for m in improved_result.matched_materials[:3]:\n",
    "                print(f\"      - {m.pk[:35]}... (conf: {m.confidence:.2f})\")\n",
    "        else:\n",
    "            print(f\"   ❌ Status: {improved_result.status} (no matches)\")\n",
    "        \n",
    "        # เช็คความถูกต้อง\n",
    "        if test_result['has_answer']:\n",
    "            llm_found_answer = test_result['answer_pk'] in result_dict['matched_pks']\n",
    "            result_dict['llm_correct'] = llm_found_answer\n",
    "            \n",
    "            if llm_found_answer:\n",
    "                # หา confidence ของ answer\n",
    "                answer_match = next((m for m in improved_result.matched_materials \n",
    "                                    if m.pk == test_result['answer_pk']), None)\n",
    "                if answer_match:\n",
    "                    print(f\"   🎯 CORRECT! Found answer (confidence: {answer_match.confidence:.2f})\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ MISSED answer: {test_result['answer_pk'][:30]}...\")\n",
    "        else:\n",
    "            result_dict['llm_correct'] = None\n",
    "        \n",
    "        improved_llm_results.append(result_dict)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {e}\")\n",
    "        result_dict = {\n",
    "            \"case_number\": test_result['case_number'],\n",
    "            \"query_pk\": test_result['query_pk'],\n",
    "            \"has_answer\": test_result['has_answer'],\n",
    "            \"answer_pk\": test_result['answer_pk'],\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        improved_llm_results.append(result_dict)\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✅ Improved LLM Complete: {len(improved_llm_results)}/{len(test_results)} cases\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63a4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
