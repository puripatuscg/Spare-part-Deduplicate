{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675ea1b8",
   "metadata": {},
   "source": [
    "# Spart-part Deduplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b2fc64",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebdc0e",
   "metadata": {},
   "source": [
    "Spare-Part Deduplication ‡∏î‡πâ‡∏ß‡∏¢ Hybrid RAG ‡πÅ‡∏•‡∏∞ Graph-based Grouping\n",
    "\n",
    "‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ Spare-Part Deduplication ‡∏°‡∏µ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ material / spare-part ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏´‡∏±‡∏™ (PK) ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á ‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ material ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢ PK ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠ ‡∏Ñ‡∏•‡∏±‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°\n",
    "\n",
    "‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (Solution Approach)\n",
    "\n",
    "‡πÇ‡∏ã‡∏•‡∏π‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î Retrieval-Augmented Generation (RAG) ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Hybrid Search ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á candidate ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô material ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ä‡πâ Large Language Model (LLM) ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (semantic reasoning) ‡∏ß‡πà‡∏≤ candidate ‡πÉ‡∏î‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô duplicate\n",
    "\n",
    "‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ö‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2edec",
   "metadata": {},
   "source": [
    "1. Hybrid Search (Candidate Retrieval)\n",
    "\n",
    "‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏î‡∏∂‡∏á candidate material ‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏ú‡∏™‡∏° (Hybrid Search) ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢:\n",
    "\n",
    "Semantic Search (Vector Search)\n",
    "‡πÉ‡∏ä‡πâ embedding ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ material ‡πÅ‡∏°‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥\n",
    "\n",
    "Keyword Search (BM25 / Full-text search)\n",
    "‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏ö keyword ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÄ‡∏ä‡πà‡∏ô ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô ‡∏£‡∏´‡∏±‡∏™‡∏£‡∏∏‡πà‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ\n",
    "\n",
    "Trigram Fuzzy Matching\n",
    "‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏™‡∏∞‡∏Å‡∏î‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©\n",
    "\n",
    "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å normalize ‡πÅ‡∏•‡∏∞‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (weighted scoring) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å candidate ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59d258",
   "metadata": {},
   "source": [
    "2. LLM-based Grouping (Duplicate Decision)\n",
    "\n",
    "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ candidate ‡∏à‡∏≤‡∏Å Hybrid Search ‡πÅ‡∏•‡πâ‡∏ß ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤:\n",
    "\n",
    "material ‡πÉ‡∏î‡πÄ‡∏õ‡πá‡∏ô duplicate ‡∏Ç‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏±‡∏ô\n",
    "\n",
    "material ‡πÉ‡∏î‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏•‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°\n",
    "\n",
    "LLM ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° material ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡πÅ‡∏°‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88180027",
   "metadata": {},
   "source": [
    "3. Graph-based Connectivity Grouping\n",
    "\n",
    "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å LLM ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô Graph ‡πÇ‡∏î‡∏¢:\n",
    "\n",
    "material ‡πÅ‡∏ï‡πà‡∏•‡∏∞ PK ‡∏Ñ‡∏∑‡∏≠ node\n",
    "\n",
    "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ß‡πà‡∏≤ ‚Äú‡πÄ‡∏õ‡πá‡∏ô duplicate ‡∏Å‡∏±‡∏ô‚Äù ‡∏Ñ‡∏∑‡∏≠ edge\n",
    "\n",
    "‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î connected components ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡∏Å‡∏•‡∏∏‡πà‡∏° duplicate ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡πÄ‡∏ä‡πà‡∏ô:\n",
    "\n",
    "‡∏ñ‡πâ‡∏≤\n",
    "\n",
    "A = B\n",
    "\n",
    "B = C\n",
    "\n",
    "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà graph ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠\n",
    "A = B = C\n",
    "\n",
    "‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ duplicate group ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡πÅ‡∏°‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba31927",
   "metadata": {},
   "source": [
    "diagram flow: (RAG search ‚Üí LLM ‚Üí Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82157c44",
   "metadata": {},
   "source": [
    "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á\n",
    "\n",
    "‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô PK ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö\n",
    "\n",
    "‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á master data\n",
    "\n",
    "‡∏•‡∏î‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏ã‡πâ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ã‡πâ‡∏≥\n",
    "\n",
    "‡∏ß‡∏≤‡∏á‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö data governance ‡πÅ‡∏•‡∏∞ master data management ‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b5653",
   "metadata": {},
   "source": [
    "## DB (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aec433",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"black_kalareemhotmail.com\",\n",
    "    \"password\": \"\",\n",
    "    \"options\": \"-c search_path=apm_dev,public -c client_encoding=UTF8\"  # Set default schema and UTF-8 encoding\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"black_kalareemhotmail.com\",\n",
    "    \"password\": \"\",\n",
    "    \"options\": \"-c search_path=apm_dev,public -c client_encoding=UTF8\"  # Set default schema and UTF-8 encoding\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"spare_part_test\"\n",
    "SCHEMA_NAME = \"apm_dev\"\n",
    "EMBEDDING_DIM = 1024  # bge-m3 dimension\n",
    "\n",
    "# ============================================================\n",
    "# Database Connection\n",
    "# ============================================================\n",
    "def get_connection():\n",
    "    \"\"\"Get database connection with pgvector support\"\"\"\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    conn.set_client_encoding('UTF8')  # Ensure UTF-8 encoding for Thai text\n",
    "    register_vector(conn)\n",
    "    return conn\n",
    "\n",
    "# ============================================================\n",
    "# 1. CREATE TABLE\n",
    "# ============================================================\n",
    "def create_spare_part_table(drop_if_exists: bool = False):\n",
    "    \"\"\"\n",
    "    Create spare_part_test table with all required fields\n",
    "    \n",
    "    Args:\n",
    "        drop_if_exists: If True, drop existing table before creating\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Drop table if requested\n",
    "        if drop_if_exists:\n",
    "            print(f\"‚ö†Ô∏è  Dropping existing table {SCHEMA_NAME}.{TABLE_NAME}...\")\n",
    "            cur.execute(f\"DROP TABLE IF EXISTS {SCHEMA_NAME}.{TABLE_NAME} CASCADE\")\n",
    "        \n",
    "        # Create table\n",
    "        create_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {SCHEMA_NAME}.{TABLE_NAME} (\n",
    "            -- Primary Key\n",
    "            pk_plant_matnum TEXT PRIMARY KEY,\n",
    "            \n",
    "            -- Basic Info\n",
    "            material_type TEXT,\n",
    "            plant TEXT,\n",
    "            material_number TEXT,\n",
    "            plant_id TEXT,\n",
    "            plant_description TEXT,\n",
    "            uom TEXT,\n",
    "            \n",
    "            -- Raw Description Fields\n",
    "            material_description TEXT,\n",
    "            po_text TEXT,\n",
    "            \n",
    "            -- ============================================================\n",
    "            -- SEMANTIC SEARCH Fields\n",
    "            -- ============================================================\n",
    "            raw_combine_for_embedding TEXT,\n",
    "            clean_combine_for_embedding TEXT,\n",
    "            embedding vector({EMBEDDING_DIM}),\n",
    "            \n",
    "            -- ============================================================\n",
    "            -- KEYWORD SEARCH Fields (BM25)\n",
    "            -- ============================================================\n",
    "            raw_combine_for_keyword TEXT,\n",
    "            clean_combine_for_keyword TEXT,\n",
    "            tsv tsvector,\n",
    "            \n",
    "            -- Extracted Keywords/Codes\n",
    "            keyword_codes TEXT[],\n",
    "            keyword_old_codes TEXT[],\n",
    "            keyword_list TEXT[],\n",
    "            \n",
    "            -- Metadata\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"üì¶ Creating table {SCHEMA_NAME}.{TABLE_NAME}...\")\n",
    "        cur.execute(create_sql)\n",
    "        \n",
    "        # Enable required extensions\n",
    "        print(\"üîß Ensuring extensions are enabled...\")\n",
    "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS pg_trgm\")\n",
    "        \n",
    "        # Create indexes\n",
    "        print(\"üîç Creating indexes...\")\n",
    "        \n",
    "        # A) Vector index for semantic search\n",
    "        cur.execute(f\"\"\"\n",
    "            DO $$\n",
    "            BEGIN\n",
    "                IF NOT EXISTS (\n",
    "                    SELECT 1 FROM pg_indexes \n",
    "                    WHERE schemaname='{SCHEMA_NAME}' \n",
    "                    AND tablename='{TABLE_NAME}' \n",
    "                    AND indexname='idx_{TABLE_NAME}_embedding'\n",
    "                ) THEN\n",
    "                    CREATE INDEX idx_{TABLE_NAME}_embedding \n",
    "                    ON {SCHEMA_NAME}.{TABLE_NAME} \n",
    "                    USING ivfflat (embedding vector_cosine_ops)\n",
    "                    WITH (lists = 100);\n",
    "                END IF;\n",
    "            END\n",
    "            $$;\n",
    "        \"\"\")\n",
    "        \n",
    "        # B) Full-text search index\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_tsv \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} \n",
    "            USING gin(tsv);\n",
    "        \"\"\")\n",
    "        \n",
    "        # C) Trigram index\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_pk_trgm \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} \n",
    "            USING gin (pk_plant_matnum gin_trgm_ops);\n",
    "        \"\"\")\n",
    "        \n",
    "        # D) Array indexes for keyword matching\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_keyword_codes \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} \n",
    "            USING gin (keyword_codes);\n",
    "            \n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_keyword_list \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} \n",
    "            USING gin (keyword_list);\n",
    "        \"\"\")\n",
    "        \n",
    "        # E) Basic indexes\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_material_type \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} (material_type);\n",
    "            \n",
    "            CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_uom \n",
    "            ON {SCHEMA_NAME}.{TABLE_NAME} (uom);\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create trigger for auto-updating tsv\n",
    "        print(\"‚öôÔ∏è  Creating trigger for tsvector auto-update...\")\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE OR REPLACE FUNCTION {SCHEMA_NAME}.{TABLE_NAME}_tsv_trigger() \n",
    "            RETURNS TRIGGER AS $$\n",
    "            BEGIN\n",
    "                NEW.tsv := to_tsvector('simple', COALESCE(NEW.clean_combine_for_keyword, ''));\n",
    "                NEW.updated_at := CURRENT_TIMESTAMP;\n",
    "                RETURN NEW;\n",
    "            END;\n",
    "            $$ LANGUAGE plpgsql;\n",
    "            \n",
    "            DROP TRIGGER IF EXISTS tsv_update_trigger ON {SCHEMA_NAME}.{TABLE_NAME};\n",
    "            \n",
    "            CREATE TRIGGER tsv_update_trigger \n",
    "            BEFORE INSERT OR UPDATE ON {SCHEMA_NAME}.{TABLE_NAME}\n",
    "            FOR EACH ROW \n",
    "            EXECUTE FUNCTION {SCHEMA_NAME}.{TABLE_NAME}_tsv_trigger();\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"‚úÖ Table {SCHEMA_NAME}.{TABLE_NAME} created successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"‚ùå Error creating table: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "# Test\n",
    "print(\"=\"*60)\n",
    "print(\"TEST: Create Table\")\n",
    "print(\"=\"*60)\n",
    "# get_connection()\n",
    "# create_spare_part_table(drop_if_exists=True)  # Uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ab396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. EMBEDDER CLASS (BGE-M3)\n",
    "# ============================================================\n",
    "class BGE_M3_Embedder:\n",
    "    \"\"\"\n",
    "    BGE-M3 Embedding Model Wrapper\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-m3\", device: str = None):\n",
    "        \"\"\"\n",
    "        Initialize BGE-M3 model\n",
    "        \n",
    "        Args:\n",
    "            model_name: Model identifier (default: BAAI/bge-m3)\n",
    "            device: 'cuda', 'cpu', or None (auto-detect)\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Loading embedding model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.dim = 1024  # bge-m3 output dimension\n",
    "        print(f\"‚úÖ Model loaded! Dimension: {self.dim}\")\n",
    "    \n",
    "    def embed_texts(\n",
    "        self, \n",
    "        texts: List[str], \n",
    "        batch_size: int = 32,\n",
    "        show_progress: bool = True\n",
    "    ) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            batch_size: Batch size for encoding\n",
    "            show_progress: Show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            List of embedding vectors (each is list of floats)\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=show_progress,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True  # Recommended for bge-m3\n",
    "        )\n",
    "        \n",
    "        return [emb.tolist() for emb in embeddings]\n",
    "    \n",
    "    def embed_single(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single text (convenience method)\"\"\"\n",
    "        return self.embed_texts([text], show_progress=False)[0]\n",
    "\n",
    "# Test\n",
    "print(\"=\"*60)\n",
    "print(\"TEST: BGE-M3 Embedder\")\n",
    "print(\"=\"*60)\n",
    "# embedder = BGE_M3_Embedder()  # Uncomment to test\n",
    "# test_emb = embedder.embed_single(\"BEARING BALL TEST\")\n",
    "# print(f\"Embedding dim: {len(test_emb)}\")\n",
    "# print(f\"First 5 values: {test_emb[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89019533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. INSERT DATA WITH EMBEDDINGS\n",
    "# ============================================================\n",
    "def insert_spare_parts(\n",
    "    df: pd.DataFrame, \n",
    "    embedder: BGE_M3_Embedder, \n",
    "    batch_size: int = 128,\n",
    "    start_from: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Insert spare part data into database with embeddings\n",
    "    \n",
    "    Required DataFrame columns:\n",
    "        - pk_plant_matnum (PRIMARY KEY)\n",
    "        - material_type, plant, material_number, plant_id, plant_description, uom\n",
    "        - material_description, po_text\n",
    "        - raw_combine_for_embedding, clean_combine_for_embedding\n",
    "        - raw_combine_for_keyword, clean_combine_for_keyword\n",
    "        - keyword_codes (list), keyword_old_codes (list), keyword_list (list)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with spare part data\n",
    "        embedder: BGE_M3_Embedder instance\n",
    "        batch_size: Number of records to process per batch\n",
    "        start_from: Skip first N records (for resuming)\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Normalize column names to lowercase\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Required columns check\n",
    "    required_cols = [\n",
    "        'pk_plant_matnum', 'material_type', 'plant', 'material_number', \n",
    "        'plantid', 'plant_description', 'uom', 'material_description', 'text',\n",
    "        'raw_combine_for_embedding', 'clean_combine_for_embedding',\n",
    "        'raw_combine_for_keyword', 'clean_combine_for_keyword',\n",
    "        'keyword_codes', 'keyword_old_codes', 'keyword_list'\n",
    "    ]\n",
    "    \n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    df_to_insert = df[required_cols].copy()\n",
    "    total_rows = len(df_to_insert)\n",
    "    \n",
    "    if start_from > 0:\n",
    "        df_to_insert = df_to_insert.iloc[start_from:]\n",
    "        print(f\"‚è≠Ô∏è  Skipping first {start_from} records...\")\n",
    "    \n",
    "    print(f\"üìä Total records to insert: {len(df_to_insert)}\")\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "        INSERT INTO {SCHEMA_NAME}.{TABLE_NAME} (\n",
    "            pk_plant_matnum, material_type, plant, material_number,\n",
    "            plant_id, plant_description, uom, material_description, po_text,\n",
    "            raw_combine_for_embedding, clean_combine_for_embedding, embedding,\n",
    "            raw_combine_for_keyword, clean_combine_for_keyword,\n",
    "            keyword_codes, keyword_old_codes, keyword_list\n",
    "        ) VALUES (\n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n",
    "        )\n",
    "        ON CONFLICT (pk_plant_matnum) \n",
    "        DO UPDATE SET\n",
    "            material_type = EXCLUDED.material_type,\n",
    "            material_description = EXCLUDED.material_description,\n",
    "            clean_combine_for_embedding = EXCLUDED.clean_combine_for_embedding,\n",
    "            clean_combine_for_keyword = EXCLUDED.clean_combine_for_keyword,\n",
    "            embedding = EXCLUDED.embedding,\n",
    "            keyword_codes = EXCLUDED.keyword_codes,\n",
    "            keyword_list = EXCLUDED.keyword_list,\n",
    "            updated_at = CURRENT_TIMESTAMP\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        inserted = 0\n",
    "        failed = 0\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(df_to_insert), batch_size), desc=\"Inserting batches\"):\n",
    "            batch_df = df_to_insert.iloc[i:i+batch_size]\n",
    "            \n",
    "            # Generate embeddings for this batch\n",
    "            texts_to_embed = batch_df['clean_combine_for_embedding'].fillna(\"\").tolist()\n",
    "            embeddings = embedder.embed_texts(texts_to_embed, show_progress=False)\n",
    "            \n",
    "            # Prepare batch data\n",
    "            batch_data = []\n",
    "            for idx, (_, row) in enumerate(batch_df.iterrows()):\n",
    "                # Handle column name variations\n",
    "                plant_id = row.get('plant_id') or row.get('plantid', '')\n",
    "                po_text = row.get('po_text') or row.get('text', '')\n",
    "                \n",
    "                batch_data.append((\n",
    "                    row['pk_plant_matnum'],\n",
    "                    row['material_type'],\n",
    "                    row['plant'],\n",
    "                    row['material_number'],\n",
    "                    plant_id,\n",
    "                    row['plant_description'],\n",
    "                    row['uom'],\n",
    "                    row['material_description'],\n",
    "                    po_text,\n",
    "                    row['raw_combine_for_embedding'],\n",
    "                    row['clean_combine_for_embedding'],\n",
    "                    embeddings[idx],  # embedding vector\n",
    "                    row['raw_combine_for_keyword'],\n",
    "                    row['clean_combine_for_keyword'],\n",
    "                    row['keyword_codes'] if isinstance(row['keyword_codes'], list) else [],\n",
    "                    row['keyword_old_codes'] if isinstance(row['keyword_old_codes'], list) else [],\n",
    "                    row['keyword_list'] if isinstance(row['keyword_list'], list) else []\n",
    "                ))\n",
    "            \n",
    "            # Execute batch insert\n",
    "            try:\n",
    "                psycopg2.extras.execute_batch(cur, insert_sql, batch_data, page_size=batch_size)\n",
    "                conn.commit()\n",
    "                inserted += len(batch_data)\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                failed += len(batch_data)\n",
    "                print(f\"\\n‚ùå Batch {i//batch_size + 1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n‚úÖ Insertion complete!\")\n",
    "        print(f\"   Inserted: {inserted} records\")\n",
    "        if failed > 0:\n",
    "            print(f\"   Failed: {failed} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"‚ùå Error during insertion: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "# Test\n",
    "print(\"=\"*60)\n",
    "print(\"TEST: Create Table & Insert Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "\n",
    "# # # Time: Create table\n",
    "# print(\"\\nüèóÔ∏è  Creating table...\")\n",
    "# start_time_table = time.time()\n",
    "# create_spare_part_table(drop_if_exists=True)\n",
    "# time_table = time.time() - start_time_table\n",
    "# print(f\"‚úÖ Table created in {time_table:.2f} seconds\\n\")\n",
    "\n",
    "# # Time: Insert data with embeddings\n",
    "# print(\"üì• Inserting data with embeddings...\")\n",
    "# start_time_insert = time.time()\n",
    "# embedder = BGE_M3_Embedder()\n",
    "# insert_spare_parts(sample_500, embedder, batch_size=32)\n",
    "# time_insert = time.time() - start_time_insert\n",
    "# print(f\"‚úÖ Data inserted in {time_insert:.2f} seconds\\n\")\n",
    "\n",
    "# print(f\"‚è±Ô∏è  Total time: {time_table + time_insert:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9118d8c",
   "metadata": {},
   "source": [
    "## Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4A. VECTOR SEARCH QUERY (SEMANTIC ONLY)\n",
    "# ============================================================\n",
    "\n",
    "def vector_search(\n",
    "    query_text: str,\n",
    "    embedder: BGE_M3_Embedder,\n",
    "    top_k_semantic: int = 100,\n",
    "    limit: int = 50,\n",
    "    uom_filter: Optional[str] = None,\n",
    "    material_type_filter: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform pure vector (semantic) search using embeddings only\n",
    "\n",
    "    Args:\n",
    "        query_text: Search query text (for embedding)\n",
    "        embedder: BGE_M3_Embedder instance\n",
    "        top_k_semantic: Top K results from vector search\n",
    "        limit: Final result limit\n",
    "        uom_filter: Optional UOM filter (e.g., 'PC')\n",
    "        material_type_filter: Optional material type filter\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with search results and semantic scores\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) Generate query embedding\n",
    "    # ------------------------------------------------------------\n",
    "    query_embedding = embedder.embed_single(query_text)\n",
    "\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "\n",
    "    try:\n",
    "        # ------------------------------------------------------------\n",
    "        # 2) Build optional filters\n",
    "        # ------------------------------------------------------------\n",
    "        filter_clause = \"\"\n",
    "        if uom_filter:\n",
    "            filter_clause += f\" AND uom = '{uom_filter}'\"\n",
    "        if material_type_filter:\n",
    "            filter_clause += f\" AND material_type = '{material_type_filter}'\"\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 3) Vector search SQL\n",
    "        # ------------------------------------------------------------\n",
    "        search_sql = f\"\"\"\n",
    "        WITH semantic_search AS (\n",
    "            SELECT\n",
    "                pk_plant_matnum,\n",
    "                material_description,\n",
    "                uom,\n",
    "                (1 - (embedding <=> %s::vector)) AS semantic_score\n",
    "            FROM {SCHEMA_NAME}.{TABLE_NAME}\n",
    "            WHERE embedding IS NOT NULL\n",
    "                {filter_clause}\n",
    "            ORDER BY embedding <=> %s::vector\n",
    "            LIMIT {top_k_semantic}\n",
    "        ),\n",
    "\n",
    "        normalized_scores AS (\n",
    "            SELECT\n",
    "                pk_plant_matnum,\n",
    "                material_description,\n",
    "                uom,\n",
    "                semantic_score,\n",
    "                CASE\n",
    "                    WHEN semantic_score > 0\n",
    "                        THEN semantic_score / MAX(semantic_score) OVER ()\n",
    "                    ELSE 0\n",
    "                END AS semantic_norm\n",
    "            FROM semantic_search\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "            pk_plant_matnum,\n",
    "            material_description,\n",
    "            uom,\n",
    "            semantic_norm AS final_score,\n",
    "            ROUND(semantic_norm::numeric, 3) AS semantic_score\n",
    "        FROM normalized_scores\n",
    "        ORDER BY final_score DESC\n",
    "        LIMIT {limit};\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # 4) Execute query\n",
    "        # ------------------------------------------------------------\n",
    "        cur.execute(search_sql, (\n",
    "            query_embedding,\n",
    "            query_embedding\n",
    "        ))\n",
    "\n",
    "        results = cur.fetchall()\n",
    "        df_results = pd.DataFrame(results)\n",
    "        return df_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Vector search error: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185655be",
   "metadata": {},
   "source": [
    "### Evaluate vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATION: VECTOR SEARCH QUALITY\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_vector_search_quality(\n",
    "    edges_df,\n",
    "    sample_df,\n",
    "    embedder,\n",
    "    top_k=30\n",
    "):\n",
    "    \"\"\"\n",
    "    ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á vector (semantic) search\n",
    "    ‡πÇ‡∏î‡∏¢‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏¢‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà rank ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà\n",
    "\n",
    "    Args:\n",
    "        edges_df: DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ Key_mat_1, Key_mat_2 (‡∏Ñ‡∏π‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏â‡∏•‡∏¢)\n",
    "        sample_df: DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• material ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (sample_500)\n",
    "        embedder: BGE_M3_Embedder instance\n",
    "        top_k: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô candidates ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤\n",
    "\n",
    "    Returns:\n",
    "        DataFrame ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) Build answer map (pk -> list of correct answers)\n",
    "    # ------------------------------------------------------------\n",
    "    answer_map = {}\n",
    "    for _, row in edges_df.iterrows():\n",
    "        key1 = row['Key_mat_1']\n",
    "        key2 = row['Key_mat_2']\n",
    "\n",
    "        if pd.notna(key1) and pd.notna(key2):\n",
    "            answer_map.setdefault(key1, []).append(key2)\n",
    "            answer_map.setdefault(key2, []).append(key1)\n",
    "\n",
    "    print(f\"üìä Total PKs with answers: {len(answer_map)}\")\n",
    "    print(f\"üîç Starting VECTOR search evaluation\")\n",
    "    print(f\"‚è±Ô∏è  This may take a few minutes...\\n\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2) Helper: case-insensitive column getter\n",
    "    # ------------------------------------------------------------\n",
    "    def get_col(rec, col_name):\n",
    "        col_lower = col_name.lower()\n",
    "        for col in rec.index:\n",
    "            if col.lower() == col_lower:\n",
    "                return rec[col]\n",
    "        return ''\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3) Evaluate each PK\n",
    "    # ------------------------------------------------------------\n",
    "    for pk, answers in tqdm(answer_map.items(), desc=\"Evaluating PKs (Vector Search)\"):\n",
    "        record_rows = sample_df[sample_df['pk_plant_matnum'] == pk]\n",
    "\n",
    "        if len(record_rows) == 0:\n",
    "            continue\n",
    "\n",
    "        record = record_rows.iloc[0]\n",
    "\n",
    "        query_text = get_col(record, 'clean_combine_for_embedding')\n",
    "        material_desc = get_col(record, 'material_description')\n",
    "\n",
    "        if not query_text or pd.isna(query_text):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # ----------------------------------------------------\n",
    "            # Vector search\n",
    "            # ----------------------------------------------------\n",
    "            candidates = vector_search(\n",
    "                query_text=query_text,\n",
    "                embedder=embedder,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            candidate_pks = candidates['pk_plant_matnum'].tolist()\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # Find ranks of correct answers\n",
    "            # ----------------------------------------------------\n",
    "            found_ranks = []\n",
    "            for answer_pk in answers:\n",
    "                if answer_pk in candidate_pks:\n",
    "                    rank = candidate_pks.index(answer_pk) + 1\n",
    "                    found_ranks.append(rank)\n",
    "\n",
    "            if found_ranks:\n",
    "                best_rank = min(found_ranks)\n",
    "                status = \"‚úÖ Found\"\n",
    "            else:\n",
    "                best_rank = None\n",
    "                status = \"‚ùå Not Found\"\n",
    "\n",
    "            results.append({\n",
    "                'pk': pk,\n",
    "                'material_description': material_desc[:50] if material_desc else '',\n",
    "                'num_answers': len(answers),\n",
    "                'answers': ', '.join(answers),\n",
    "                'found_ranks': ', '.join(map(str, found_ranks)) if found_ranks else 'N/A',\n",
    "                'best_rank': best_rank,\n",
    "                'status': status\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Error processing {pk}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4) Results DataFrame\n",
    "    # ------------------------------------------------------------\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EVALUATION: Vector Search Quality Assessment\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "start_time_eval = time.time()\n",
    "\n",
    "embedder = BGE_M3_Embedder()\n",
    "\n",
    "evaluation_results_vec = evaluate_vector_search_quality(\n",
    "    edges_df=sample100,\n",
    "    sample_df=sample_500,\n",
    "    embedder=embedder,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "time_eval = time.time() - start_time_eval\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation completed in {time_eval:.2f} seconds\")\n",
    "print(f\"‚ö° Average time per query: {time_eval/len(evaluation_results_vec):.3f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06daf3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SUMMARY STATISTICS: VECTOR SEARCH\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà SUMMARY STATISTICS (VECTOR SEARCH)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total = len(evaluation_results_vec)\n",
    "found = len(evaluation_results_vec[evaluation_results_vec['status'] == '‚úÖ Found'])\n",
    "not_found = total - found\n",
    "\n",
    "print(f\"Total PKs evaluated: {total}\")\n",
    "print(f\"Found in candidates: {found} ({found/total*100:.1f}%)\")\n",
    "print(f\"Not found: {not_found} ({not_found/total*100:.1f}%)\")\n",
    "\n",
    "if found > 0:\n",
    "    found_df = evaluation_results_vec[\n",
    "        evaluation_results_vec['status'] == '‚úÖ Found'\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nRank Statistics (for found answers):\")\n",
    "    print(f\"  Mean Rank: {found_df['best_rank'].mean():.2f}\")\n",
    "    print(f\"  Median Rank: {found_df['best_rank'].median():.1f}\")\n",
    "    print(f\"  Best Rank: {found_df['best_rank'].min()}\")\n",
    "    print(f\"  Worst Rank: {found_df['best_rank'].max()}\")\n",
    "\n",
    "    total_found = len(found_df)\n",
    "\n",
    "    top_1 = len(found_df[found_df['best_rank'] == 1])\n",
    "    top_2_3 = len(found_df[(found_df['best_rank'] >= 2) & (found_df['best_rank'] <= 3)])\n",
    "    top_4_5 = len(found_df[(found_df['best_rank'] >= 4) & (found_df['best_rank'] <= 5)])\n",
    "    top_6_10 = len(found_df[(found_df['best_rank'] >= 6) & (found_df['best_rank'] <= 10)])\n",
    "    top_11_20 = len(found_df[(found_df['best_rank'] >= 11) & (found_df['best_rank'] <= 20)])\n",
    "    top_21_25 = len(found_df[(found_df['best_rank'] >= 21) & (found_df['best_rank'] <= 25)])\n",
    "    top_26_30 = len(found_df[(found_df['best_rank'] >= 26) & (found_df['best_rank'] <= 30)])\n",
    "    top_31_40 = len(found_df[(found_df['best_rank'] >= 31) & (found_df['best_rank'] <= 40)])\n",
    "    over_40 = len(found_df[found_df['best_rank'] > 40])\n",
    "\n",
    "    print(f\"\\nüìä Rank Distribution:\")\n",
    "    print(f\"  Top 1: {top_1}\")\n",
    "    print(f\"  Rank 2‚Äì3: {top_2_3}\")\n",
    "    print(f\"  Rank 4‚Äì5: {top_4_5}\")\n",
    "    print(f\"  Rank 6‚Äì10: {top_6_10}\")\n",
    "    print(f\"  Rank 11‚Äì20: {top_11_20}\")\n",
    "    print(f\"  Rank 21‚Äì25: {top_21_25}\")\n",
    "    print(f\"  Rank 26‚Äì30: {top_26_30}\")\n",
    "    print(f\"  Rank 31‚Äì40: {top_31_40}\")\n",
    "    print(f\"  Over 40: {over_40}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\n‚úÖ Total counted: \"\n",
    "        f\"{top_1 + top_2_3 + top_4_5 + top_6_10 + top_11_20 + top_21_25 + top_26_30 + top_31_40 + over_40}\"\n",
    "    )\n",
    "    print(f\"üì¶ Total rows: {total_found}\")\n",
    "\n",
    "# ============================================================\n",
    "# CASES WHERE ANSWER NOT FOUND\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ùå CASES WHERE ANSWER NOT FOUND (VECTOR SEARCH)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "not_found_df = evaluation_results_vec[\n",
    "    evaluation_results_vec['status'] == '‚ùå Not Found'\n",
    "]\n",
    "\n",
    "if len(not_found_df) > 0:\n",
    "    print(\n",
    "        not_found_df[['pk', 'material_description', 'answers']]\n",
    "        .to_string(index=False)\n",
    "    )\n",
    "else:\n",
    "    print(\"üéâ All answers were found in candidates!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc373ff",
   "metadata": {},
   "source": [
    "## LLM-Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b08c4",
   "metadata": {},
   "source": [
    "### ‚úÖ LLM Material Grouping - UPDATED VERSION\n",
    "\n",
    "**‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á:**\n",
    "\n",
    "1. **Output Schema ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢** - ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô:\n",
    "   - `status`: \"match\" ‡∏´‡∏£‡∏∑‡∏≠ \"unmatch\" \n",
    "   - `matched_pks`: list ‡∏Ç‡∏≠‡∏á PKs ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô duplicate (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° query_pk ‡πÄ‡∏≠‡∏á)\n",
    "   - `summary`: ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ\n",
    "   \n",
    "2. **Prompt ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô** - ‡∏ö‡∏≠‡∏Å LLM ‡πÉ‡∏´‡πâ return ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ PKs ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á analysis ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß\n",
    "\n",
    "3. **Validation ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á** - ‡πÉ‡∏ä‡πâ PKs ‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å edges.csv:\n",
    "   - 7560_75060675913234 (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 7560_75060670873346)\n",
    "   - PO11_PO06131335013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.genai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d49854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPROVED LLM Function with Step-by-Step Reasoning\n",
    "# ============================================================\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class MatchedMaterial(BaseModel):\n",
    "    \"\"\"Material ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö query\"\"\"\n",
    "    pk: str = Field(description=\"PK ‡∏Ç‡∏≠‡∏á candidate ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô duplicate\")\n",
    "    confidence: float = Field(description=\"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à 0.0-1.0\", ge=0.0, le=1.0)\n",
    "    reason: str = Field(description=\"‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô\")\n",
    "\n",
    "class ImprovedMaterialGroupingResult(BaseModel):\n",
    "    \"\"\"‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° materials ‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á\"\"\"\n",
    "    query_pk: str\n",
    "    status: str  # \"match\" ‡∏´‡∏£‡∏∑‡∏≠ \"unmatch\"\n",
    "    matched_materials: List[MatchedMaterial] = Field(default_factory=list)\n",
    "    step_by_step_reasoning: str  # Reasoning ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô\n",
    "    summary: str\n",
    "\n",
    "def improved_group_materials_with_llm(\n",
    "    query_pk: str,\n",
    "    query_description: str,\n",
    "    query_full_text: str,\n",
    "    candidates: pd.DataFrame,\n",
    "    few_shot_examples: List[dict],\n",
    "    model: str = \"gemini-3-flash-preview\",\n",
    "    min_confidence: float = 0.7\n",
    ") -> ImprovedMaterialGroupingResult:\n",
    "    \"\"\"\n",
    "    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á LLM grouping ‡∏î‡πâ‡∏ß‡∏¢ step-by-step reasoning ‡πÅ‡∏•‡∏∞ few-shot examples\n",
    "    \"\"\"\n",
    "    \n",
    "    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° client\n",
    "    client = genai.Client(\n",
    "        vertexai={\n",
    "            \"project\": \"prj-service-mlops\",\n",
    "            \"location\": \"global\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° candidates\n",
    "    candidate_list = []\n",
    "    for idx, row in candidates.iterrows():\n",
    "        candidate_list.append({\n",
    "            \"pk\": row['pk_plant_matnum'],\n",
    "            \"description\": row['material_description'],\n",
    "            \"full_text\": row.get('clean_combine_for_keyword', row['material_description'])[:500]\n",
    "        })\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Few-Shot Examples section\n",
    "    examples_text = \"\"\n",
    "    for i, ex in enumerate(few_shot_examples, 1):\n",
    "        examples_text += f\"\"\"\n",
    "### Example {i}: Materials that ARE Duplicates\n",
    "\n",
    "**Query Material:**\n",
    "- PK: {ex['query_pk']}\n",
    "- Description: {ex['query_desc']}\n",
    "\n",
    "**Candidate Material (DUPLICATE):**\n",
    "- PK: {ex['answer_pk']}\n",
    "- Description: {ex['answer_desc']}\n",
    "\n",
    "**Why they are duplicates:** Despite different wording, they refer to the same physical product. Key identifiers and specifications match.\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡πÉ‡∏´‡∏°‡πà\n",
    "    prompt = f\"\"\"# Advanced Material Deduplication with Step-by-Step Analysis\n",
    "\n",
    "You are an expert in industrial spare parts identification. Your task is to identify which candidate materials are EXACT DUPLICATES of the query material using systematic analysis.\n",
    "\n",
    "## üìã Few-Shot Examples (Learn from these):\n",
    "{examples_text}\n",
    "\n",
    "## üéØ Current Task:\n",
    "\n",
    "### Query Material (Find duplicates of this):\n",
    "- **PK**: {query_pk}\n",
    "- **Description**: {query_description}\n",
    "- **Full Text**: {query_full_text[:800]}\n",
    "\n",
    "### Candidate Materials (Analyze each one):\n",
    "\"\"\"\n",
    "    \n",
    "    for i, cand in enumerate(candidate_list, 1):\n",
    "        prompt += f\"\"\"\n",
    "**Candidate #{i}:**\n",
    "- PK: {cand['pk']}\n",
    "- Description: {cand['description']}\n",
    "- Full Text: {cand['full_text']}\n",
    "\"\"\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "\n",
    "## üîç Analysis Framework (Follow these steps):\n",
    "\n",
    "**Step 1: Extract Key Identifiers**\n",
    "- Part numbers, model codes, manufacturer codes\n",
    "- Size/dimensions (numbers with units like mm, cm, inch)\n",
    "- Material type (steel, plastic, rubber, etc.)\n",
    "- Capacity/rating (voltage, pressure, flow rate)\n",
    "\n",
    "**Step 2: Compare Technical Specifications**\n",
    "- Are dimensions identical or equivalent?\n",
    "- Same material composition?\n",
    "- Same technical ratings?\n",
    "- Same functionality?\n",
    "\n",
    "**Step 3: Evaluate Semantic Similarity**\n",
    "- Do descriptions refer to the same product?\n",
    "- Are they used for the same purpose?\n",
    "- Same application area?\n",
    "\n",
    "**Step 4: Determine Confidence Level**\n",
    "- **High (0.9-1.0)**: Identical part numbers AND specs\n",
    "- **Medium-High (0.8-0.89)**: Same specs, similar codes\n",
    "- **Medium (0.7-0.79)**: Same specs, different codes\n",
    "- **Below 0.7**: Uncertain or different materials\n",
    "\n",
    "**Step 5: Make Final Decision**\n",
    "- Only include matches with confidence ‚â• 0.7\n",
    "- When uncertain, be conservative (don't match)\n",
    "\n",
    "## ‚úÖ Matching Rules:\n",
    "\n",
    "**MATCH if:**\n",
    "- Same part number/model code (even with plant prefix)\n",
    "- Same specifications (size, material, capacity)\n",
    "- Different only in: plant code, vendor ID, packaging info\n",
    "\n",
    "**DO NOT MATCH if:**\n",
    "- Different part numbers AND different specs\n",
    "- Different sizes or capacities\n",
    "- Different materials or types\n",
    "- One is accessory, another is main product\n",
    "- Similar but not identical\n",
    "\n",
    "## üìä Output Format:\n",
    "\n",
    "For each duplicate found, provide:\n",
    "1. **pk**: The candidate PK\n",
    "2. **confidence**: 0.7-1.0 (‚â•0.7 to include)\n",
    "3. **reason**: Brief explanation (30-50 words)\n",
    "\n",
    "Include **step_by_step_reasoning** showing your analysis process.\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes:\n",
    "\n",
    "- Be systematic - analyze ALL candidates\n",
    "- Be conservative - only high confidence matches\n",
    "- Ignore the first candidate if it's the query itself (same PK)\n",
    "- Focus on TECHNICAL match, not just keyword similarity\n",
    "- Different plants can have same material\n",
    "\n",
    "Analyze carefully and provide your results.\"\"\"\n",
    "\n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=ImprovedMaterialGroupingResult,\n",
    "            temperature=0.1,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô\n",
    "            top_p=0.95\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    result = ImprovedMaterialGroupingResult.model_validate_json(response.text)\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    filtered_matches = [\n",
    "        m for m in result.matched_materials \n",
    "        if m.confidence >= min_confidence\n",
    "    ]\n",
    "    \n",
    "    result.matched_materials = filtered_matches\n",
    "    if not filtered_matches:\n",
    "        result.status = \"unmatch\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Improved LLM function created\")\n",
    "print(f\"   Model: gemini-3-flash-preview\")\n",
    "print(f\"   Min confidence: 0.7\")\n",
    "print(f\"   Features: Step-by-step reasoning + Few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6facf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Improved LLM ‡∏Å‡∏±‡∏ö Test Cases\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ TESTING IMPROVED LLM (gemini-3-flash-preview)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "improved_llm_results = []\n",
    "\n",
    "for i, test_result in enumerate(test_results, 1):\n",
    "    print(f\"\\n[{i}/{len(test_results)}] Case {test_result['case_number']}: {test_result['query_pk'][:40]}...\")\n",
    "    \n",
    "    # ‡∏î‡∏∂‡∏á query info\n",
    "    query_pk = test_result['query_pk']\n",
    "    query_description = test_result['query_description']\n",
    "    \n",
    "    query_record = sample_500[sample_500['pk_plant_matnum'] == query_pk].iloc[0]\n",
    "    query_full_text = query_record.get('clean_combine_for_keyword', query_description)\n",
    "    \n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á candidates ‡πÄ‡∏õ‡πá‡∏ô DataFrame\n",
    "    candidates_df = pd.DataFrame(test_result['candidates'])\n",
    "    \n",
    "    try:\n",
    "        # ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Improved LLM\n",
    "        print(f\"   üì§ Sending to improved LLM...\")\n",
    "        \n",
    "        improved_result = improved_group_materials_with_llm(\n",
    "            query_pk=query_pk,\n",
    "            query_description=query_description,\n",
    "            query_full_text=query_full_text,\n",
    "            candidates=candidates_df,\n",
    "            few_shot_examples=selected_examples,\n",
    "            model=\"gemini-3-flash-preview\",\n",
    "            min_confidence=0.7\n",
    "        )\n",
    "        \n",
    "        # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "        result_dict = {\n",
    "            \"case_number\": test_result['case_number'],\n",
    "            \"query_pk\": test_result['query_pk'],\n",
    "            \"has_answer\": test_result['has_answer'],\n",
    "            \"answer_pk\": test_result['answer_pk'],\n",
    "            \"answer_in_candidates\": test_result.get('answer_in_candidates'),\n",
    "            \"answer_rank\": test_result.get('answer_rank'),\n",
    "            \"llm_response\": improved_result,\n",
    "            \"matched_pks\": [m.pk for m in improved_result.matched_materials]\n",
    "        }\n",
    "        \n",
    "        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "        if improved_result.status == \"match\" and improved_result.matched_materials:\n",
    "            print(f\"   ‚úÖ Status: {improved_result.status}\")\n",
    "            print(f\"   üìã Found {len(improved_result.matched_materials)} matches:\")\n",
    "            for m in improved_result.matched_materials[:3]:\n",
    "                print(f\"      - {m.pk[:35]}... (conf: {m.confidence:.2f})\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Status: {improved_result.status} (no matches)\")\n",
    "        \n",
    "        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
    "        if test_result['has_answer']:\n",
    "            llm_found_answer = test_result['answer_pk'] in result_dict['matched_pks']\n",
    "            result_dict['llm_correct'] = llm_found_answer\n",
    "            \n",
    "            if llm_found_answer:\n",
    "                # ‡∏´‡∏≤ confidence ‡∏Ç‡∏≠‡∏á answer\n",
    "                answer_match = next((m for m in improved_result.matched_materials \n",
    "                                    if m.pk == test_result['answer_pk']), None)\n",
    "                if answer_match:\n",
    "                    print(f\"   üéØ CORRECT! Found answer (confidence: {answer_match.confidence:.2f})\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è MISSED answer: {test_result['answer_pk'][:30]}...\")\n",
    "        else:\n",
    "            result_dict['llm_correct'] = None\n",
    "        \n",
    "        improved_llm_results.append(result_dict)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        result_dict = {\n",
    "            \"case_number\": test_result['case_number'],\n",
    "            \"query_pk\": test_result['query_pk'],\n",
    "            \"has_answer\": test_result['has_answer'],\n",
    "            \"answer_pk\": test_result['answer_pk'],\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        improved_llm_results.append(result_dict)\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Improved LLM Complete: {len(improved_llm_results)}/{len(test_results)} cases\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63a4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
